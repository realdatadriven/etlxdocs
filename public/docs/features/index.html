<!doctype html><html lang=en-GB><head><meta charset=utf-8><title>Features | ETLX</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A guide to the core features of the ETLX framework."><meta name=keywords content="Documentation,Hugo,Hugo Theme,Bootstrap"><meta name=author content="Colin Wilson - Lotus Labs"><meta name=email content="support@aigis.uk"><meta name=website content="https://lotusdocs.dev"><meta name=Version content="v0.1.0"><link rel=icon href=https://realdatadriven.github.io/etlxdocs/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=https://realdatadriven.github.io/etlxdocs/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=https://realdatadriven.github.io/etlxdocs/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://realdatadriven.github.io/etlxdocs/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://realdatadriven.github.io/etlxdocs/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://realdatadriven.github.io/etlxdocs/site.webmanifest><meta property="og:title" content="Features"><meta property="og:description" content="A guide to the core features of the ETLX framework."><meta property="og:type" content="website"><meta property="og:url" content="https://realdatadriven.github.io/etlxdocs/docs/features/"><meta property="og:image" content="https://realdatadriven.github.io/etlxdocs/opengraph/card-base-2_hu_913b0cdaa6df4c5f.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://realdatadriven.github.io/etlxdocs/opengraph/card-base-2_hu_913b0cdaa6df4c5f.png"><meta name=twitter:title content="Features"><meta name=twitter:description content="A guide to the core features of the ETLX framework."><link rel=alternate type=application/atom+xml title="Atom feed for ETLX | ETL / ELT Framework for Data Engineering" href=/index.xml><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><script type=text/javascript src=https://realdatadriven.github.io/etlxdocs/docs/js/flexsearch.bundle.min.f5159d5a2151ffbb653996ec17eaff7da4e04c286bd879fc41839d36a5586f3f20eaead0b6089de48f9adc669cdee771.js integrity=sha384-9RWdWiFR/7tlOZbsF+r/faTgTChr2Hn8QYOdNqVYbz8g6urQtgid5I+a3Gac3udx crossorigin=anonymous></script><link rel=preconnect href=https://fonts.gstatic.com/><link rel=preconnect href=https://fonts.gstatic.com/ crossorigin><link href="https://fonts.googleapis.com/css?family=Inter:300,400,600,700|Fira+Code:500,700&display=block" rel=stylesheet><link rel=stylesheet href=/etlxdocs/docs/scss/style.min.f667656027a2bf1743e3b5561f79d3e485d54e1a03ce867bea46027110dd846ceb0b4ab42659491adb6a1536089561e0.css integrity=sha384-9mdlYCeivxdD47VWH3nT5IXVThoDzoZ76kYCcRDdhGzrC0q0JllJGttqFTYIlWHg crossorigin=anonymous><script defer data-domain=realdatadriven.github.io/etlxdocs data-api=/docs/s/api/event/ src=/docs/s/js/script.outbound-links.js></script><script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head><body><div class=content><div class="page-wrapper toggled"><nav id=sidebar class=sidebar-wrapper><div class=sidebar-brand><a href=https://realdatadriven.github.io/etlxdocs/ aria-label=HomePage alt=HomePage><svg id="Layer_1" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"/></svg></a></div><div class=sidebar-content style="height:calc(100% - 131px)"><ul class=sidebar-menu><li><a class=sidebar-root-link href=https://realdatadriven.github.io/etlxdocs/docs/overview/><i class="material-icons me-2">circle</i>
Overview</a></li><li><a class=sidebar-root-link href=https://realdatadriven.github.io/etlxdocs/docs/quickstart/><i class="material-icons me-2">rocket_launch</i>
Quickstart</a></li><li class="sidebar-dropdown current active"><button class=btn>
<i class="material-icons me-2">auto_awesome</i>
Features</button><div class="sidebar-submenu d-block"><ul><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/etl-elt/>ETL | ELT</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/validation/>ETL | ELT - Validation Rules</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/query-documentation/>Query Documentation</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/data-quality/>Data Quality</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/multi-query/>Multi / Stacked Queries</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/exports/>Exports</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/actions/>Actions</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/scripts/>Scripts</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/logs/>Logs / Observability</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/notify/>Notifications</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/require/>Requires</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/advanced/>Advanced Features</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/embedding/>Embedding in GO</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/ddb-at-the-core/>DuckDB at the Core</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/multi-engine-execution/>Multi-Engine Execution</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/beyond-etl/>Beyond ETL / ELT</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/features/api/>Go API & Programmatic Usage</a></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">layers</i>
Advanced Examples</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/advanced-examples/embedded-sqlite/>Embedded SQLite Example</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/advanced-examples/governance-artifacts/>Governance Artifacts</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/advanced-examples/load-external-dependencies/>Loading External Dependencies in ETLX</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/advanced-examples/schema-update/>Schema Update Example</a></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">heart_plus</i>
Contributing</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/contributing/how-to-contribute/>How to Contribute</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/contributing/code-of-conduct/>Code of Conduct</a></li></ul></div></li><li class=sidebar-dropdown><button class=btn>
<i class="material-icons me-2">help</i>
Help</button><div class=sidebar-submenu><ul><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/help/troubleshooting/>Troubleshooting</a></li><li><a class=sidebar-nested-link href=https://realdatadriven.github.io/etlxdocs/docs/help/faqs/>FAQ</a></li></ul></div></li></ul></div><ul class="sidebar-footer list-unstyled mb-0"></ul></nav><main class="page-content bg-transparent"><div id=top-header class="top-header d-print-none"><div class="header-bar d-flex justify-content-between"><div class="d-flex align-items-center"><a href=https://realdatadriven.github.io/etlxdocs/ class="logo-icon me-3" aria-label=HomePage alt=HomePage><div class=small><svg id="Layer_1" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"/></svg></div><div class=big><svg id="Layer_1" viewBox="0 0 250 250"><path d="m143 39.5c-18 0-18 18-18 18s0-18-18-18H22c-2.76.0-5 2.24-5 5v143c0 2.76 2.24 5 5 5h76c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h76c2.76.0 5-2.24 5-5V44.5c0-2.76-2.24-5-5-5h-85zM206 163c0 1.38-1.12 2.5-2.5 2.5H143c-18 0-18 18-18 18s0-18-18-18H46.5c-1.38.0-2.5-1.12-2.5-2.5V69c0-1.38 1.12-2.5 2.5-2.5H98c7.2.0 8.64 11.52 8.93 16.13.07 1.05.95 1.87 2 1.87h32.14c1.06.0 1.94-.82 2-1.87.29-4.61 1.73-16.13 8.93-16.13h51.5c1.38.0 2.5 1.12 2.5 2.5v94z" style="fill:#06f"/></svg></div></a><button id=close-sidebar class="btn btn-icon btn-soft">
<span class="material-icons size-20 menu-icon align-middle">menu</span>
</button>
<button id=flexsearch-button class="ms-3 btn btn-soft" data-bs-toggle=collapse data-bs-target=#FlexSearchCollapse aria-expanded=false aria-controls=FlexSearchCollapse>
<span class="material-icons size-20 menu-icon align-middle">search</span>
<span class="flexsearch-button-placeholder ms-1 me-2 d-none d-sm-block">Search</span><div class="d-none d-sm-block"><span class=flexsearch-button-keys><kbd class=flexsearch-button-cmd-key><svg width="44" height="15"><path d="M2.118 11.5A1.519 1.519.0 011 11.042 1.583 1.583.0 011 8.815a1.519 1.519.0 011.113-.458h.715V6.643h-.71A1.519 1.519.0 011 6.185 1.519 1.519.0 01.547 5.071 1.519 1.519.0 011 3.958 1.519 1.519.0 012.118 3.5a1.519 1.519.0 011.114.458A1.519 1.519.0 013.69 5.071v.715H5.4V5.071A1.564 1.564.0 016.976 3.5 1.564 1.564.0 018.547 5.071 1.564 1.564.0 016.976 6.643H6.261V8.357h.715a1.575 1.575.0 011.113 2.685 1.583 1.583.0 01-2.227.0A1.519 1.519.0 015.4 9.929V9.214H3.69v.715a1.519 1.519.0 01-.458 1.113A1.519 1.519.0 012.118 11.5zm0-.857a.714.714.0 00.715-.714V9.214H2.118a.715.715.0 100 1.429zm4.858.0a.715.715.0 100-1.429H6.261v.715a.714.714.0 00.715.714zM3.69 8.357H5.4V6.643H3.69zM2.118 5.786h.715V5.071a.714.714.0 00-.715-.714.715.715.0 00-.5 1.22A.686.686.0 002.118 5.786zm4.143.0h.715a.715.715.0 00.5-1.22.715.715.0 00-1.22.5z" fill="currentColor"/><path d="M12.4 11.475H11.344l3.879-7.95h1.056z" fill="currentColor"/><path d="M25.073 5.384l-.864.576a2.121 2.121.0 00-1.786-.923 2.207 2.207.0 00-2.266 2.326 2.206 2.206.0 002.266 2.325 2.1 2.1.0 001.782-.918l.84.617a3.108 3.108.0 01-2.622 1.293 3.217 3.217.0 01-3.349-3.317 3.217 3.217.0 013.349-3.317A3.046 3.046.0 0125.073 5.384z" fill="currentColor"/><path d="M30.993 5.142h-2.07v5.419H27.891V5.142h-2.07V4.164h5.172z" fill="currentColor"/><path d="M34.67 4.164c1.471.0 2.266.658 2.266 1.851.0 1.087-.832 1.809-2.134 1.855l2.107 2.691h-1.28L33.591 7.87H33.07v2.691H32.038v-6.4zm-1.6.969v1.8h1.572c.832.0 1.22-.3 1.22-.918s-.411-.882-1.22-.882z" fill="currentColor"/><path d="M42.883 10.561H38.31v-6.4h1.033V9.583h3.54z" fill="currentColor"/></svg>
</kbd><kbd class=flexsearch-button-key><svg width="15" height="15"><path d="M5.926 12.279H4.41L9.073 2.721H10.59z" fill="currentColor"/></svg></kbd></span></div></button></div><div class="d-flex align-items-center"><ul class="list-unstyled mb-0"><li class="list-inline-item mb-0"><a href=https://github.com/realdatadriven/etlx alt=github rel="noopener noreferrer" target=_blank><div class="btn btn-icon btn-default border-0"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>GitHub</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></div></a></li></ul><button id=mode class="btn btn-icon btn-default ms-2" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg height="30" width="30" viewBox="0 0 48 48" fill="currentColor"><title>Enable dark mode</title><path d="M24 42q-7.5.0-12.75-5.25T6 24t5.25-12.75T24 6q.4.0.85.025.45.025 1.15.075-1.8 1.6-2.8 3.95t-1 4.95q0 4.5 3.15 7.65Q28.5 25.8 33 25.8q2.6.0 4.95-.925T41.9 22.3q.05.6.075.975Q42 23.65 42 24q0 7.5-5.25 12.75T24 42zm0-3q5.45.0 9.5-3.375t5.05-7.925q-1.25.55-2.675.825Q34.45 28.8 33 28.8q-5.75.0-9.775-4.025T19.2 15q0-1.2.25-2.575t.9-3.125q-4.9 1.35-8.125 5.475Q9 18.9 9 24q0 6.25 4.375 10.625T24 39zm-.2-14.85z"/></svg>
</span><span class=toggle-light><svg height="30" width="30" viewBox="0 0 48 48" fill="currentColor"><title>Enable light mode</title><path d="M24 31q2.9.0 4.95-2.05T31 24t-2.05-4.95T24 17t-4.95 2.05T17 24t2.05 4.95T24 31zm0 3q-4.15.0-7.075-2.925T14 24t2.925-7.075T24 14t7.075 2.925T34 24t-2.925 7.075T24 34zM3.5 25.5q-.65.0-1.075-.425Q2 24.65 2 24t.425-1.075Q2.85 22.5 3.5 22.5h5q.65.0 1.075.425Q10 23.35 10 24t-.425 1.075T8.5 25.5zm36 0q-.65.0-1.075-.425Q38 24.65 38 24t.425-1.075T39.5 22.5h5q.65.0 1.075.425Q46 23.35 46 24t-.425 1.075-1.075.425zM24 10q-.65.0-1.075-.425Q22.5 9.15 22.5 8.5v-5q0-.65.425-1.075Q23.35 2 24 2t1.075.425T25.5 3.5v5q0 .65-.425 1.075Q24.65 10 24 10zm0 36q-.65.0-1.075-.425T22.5 44.5v-5q0-.65.425-1.075Q23.35 38 24 38t1.075.425.425 1.075v5q0 .65-.425 1.075Q24.65 46 24 46zM12 14.1l-2.85-2.8q-.45-.45-.425-1.075.025-.625.425-1.075.45-.45 1.075-.45t1.075.45L14.1 12q.4.45.4 1.05.0.6-.4 1-.4.45-1.025.45T12 14.1zm24.7 24.75L33.9 36q-.4-.45-.4-1.075t.45-1.025q.4-.45 1-.45t1.05.45l2.85 2.8q.45.45.425 1.075-.025.625-.425 1.075-.45.45-1.075.45t-1.075-.45zM33.9 14.1q-.45-.45-.45-1.05.0-.6.45-1.05l2.8-2.85q.45-.45 1.075-.425.625.025 1.075.425.45.45.45 1.075t-.45 1.075L36 14.1q-.4.4-1.025.4t-1.075-.4zM9.15 38.85q-.45-.45-.45-1.075t.45-1.075L12 33.9q.45-.45 1.05-.45.6.0 1.05.45.45.45.45 1.05.0.6-.45 1.05l-2.8 2.85q-.45.45-1.075.425-.625-.025-1.075-.425zM24 24z"/></svg></span></button></div></div><div class=collapse id=FlexSearchCollapse><div class=flexsearch-container><div class=flexsearch-keymap><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Arrow down" role="img"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 3.5v8m3-3-3 3-3-3"/></g></svg></kbd>
<kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Arrow up" role="img"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M7.5 11.5v-8m3 3-3-3-3 3"/></g></svg></kbd>
<span class=flexsearch-key-label>to navigate</span></li><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Enter key" role="img"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M12 3.53088v3c0 1-1 2-2 2H4m3 3-3-3 3-3"/></g></svg></kbd>
<span class=flexsearch-key-label>to select</span></li><li><kbd class=flexsearch-button-cmd-key><svg width="15" height="15" aria-label="Escape key" role="img"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.2"><path d="M13.6167 8.936c-.1065.3583-.6883.962-1.4875.962-.7993.0-1.653-.9165-1.653-2.1258v-.5678c0-1.2548.7896-2.1016 1.653-2.1016s1.3601.4778 1.4875 1.0724M9 6c-.1352-.4735-.7506-.9219-1.46-.8972-.7092.0246-1.344.57-1.344 1.2166s.4198.8812 1.3445.9805C8.465 7.3992 8.968 7.9337 9 8.5s-.454 1.398-1.4595 1.398C6.6593 9.898 6 9 5.963 8.4851m-1.4748.5368c-.2635.5941-.8099.876-1.5443.876s-1.7073-.6248-1.7073-2.204v-.4603c0-1.0416.721-2.131 1.7073-2.131.9864.0 1.6425 1.031 1.5443 2.2492h-2.956"/></g></svg></kbd>
<span class=flexsearch-key-label>to close</span></li></div><form class="flexsearch position-relative flex-grow-1 ms-2 me-2"><div class="d-flex flex-row"><input id=flexsearch class=form-control type=search placeholder=Search aria-label=Search autocomplete=off>
<button id=hideFlexsearch type=button class="ms-2 btn btn-soft">
cancel</button></div><div id=suggestions class="shadow rounded-1 d-none"></div></form></div></div></div><div class=container-fluid><div class=layout-spacing><div class="d-md-flex justify-content-between align-items-center"><nav aria-label=breadcrumb class="d-inline-block pb-2 mt-1 mt-sm-0"><ul id=breadcrumbs class="breadcrumb bg-transparent mb-0" itemscope itemtype=https://schema.org/BreadcrumbList><li class="breadcrumb-item text-capitalize active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a itemprop=item href=/etlxdocs/docs/><i class="material-icons size-20 align-text-bottom" itemprop=name>Home</i>
</a><meta itemprop=position content='1'></li><li class="breadcrumb-item text-capitalize active" itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Features</span>
<meta itemprop=position content='2'></li></ul></nav></div><div class="row flex-xl-nowrap"><div class="docs-toc col-xl-3 visually-hidden visually-hidden d-xl-block"><toc><div class="fw-bold text-uppercase mb-2">On this page</div><nav id=toc></nav></toc></div><div class="docs-toc-mobile visually-hidden visually-hidden d-print-none d-xl-none"><button id=toc-dropdown-btn class="btn-secondary dropdown-toggle" type=button data-bs-toggle=dropdown data-bs-offset=0,0 aria-expanded=false>
Table of Contents</button><nav id=toc-mobile></nav></div><div class="docs-content col-12 mt-0"><div class="mb-0 d-flex"><i class="material-icons title-icon me-2">auto_awesome</i><h1 class="content-title mb-0">Features</h1></div><p class="lead mb-3">A guide to the core features of the ETLX framework.</p><div id=content class=main-content data-bs-spy=scroll data-bs-root-margin="0px 0px -65%" data-bs-target=#toc-mobile><div class="row flex-xl-wrap"><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/etl-elt/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">auto_awesome</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">ETL | ELT</p><p class="para card-text mb-0">How ETLX models, executes, and observes ETL and ELT pipelines using declarative, metadata-driven â€¦</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/validation/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">check_circle</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">ETL | ELT - Validation Rules</p><p class="para card-text mb-0">How ETLX enables declarative data quality validation as a first-class concern of ETL / ELT execution â€¦</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/query-documentation/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">overview</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Query Documentation</p><p class="para card-text mb-0">How ETLX models SQL queries as executable documentation to improve clarity, governance, and â€¦</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/data-quality/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">check</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Data Quality</p><p class="para card-text mb-0">How ETLX models, executes, and observes data quality rules as first-class, metadata-driven â€¦</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/multi-query/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">layers</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Multi / Stacked Queries</p><p class="para card-text mb-0">Combine multiple structured queries into a single result set using UNION-based execution.</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/exports/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">download</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Exports</p><p class="para card-text mb-0">Handling data exports to files and templates using ETLX</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/actions/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">code</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Actions</p><p class="para card-text mb-0">Define file operations and external transfers in ETLX workflows</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/scripts/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">code</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Scripts</p><p class="para card-text mb-0">Execute SQL statements for operational and maintenance tasks</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/logs/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">list</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Logs / Observability</p><p class="para card-text mb-0">ETLX logging mechanism to save logs into a database.</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/notify/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">mail_outline</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Notifications</p><p class="para card-text mb-0">Send notifications with dynamic templates from SQL query results</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/require/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">arrow_right_alt</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Requires</p><p class="para card-text mb-0">Load configuration dependencies from files or queries.</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/advanced/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">auto_awesome</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Advanced Features</p><p class="para card-text mb-0">Explore advanced ETLX features like dynamic query generation, conditional execution, and modular â€¦</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/embedding/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">code</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Embedding in GO</p><p class="para card-text mb-0">Embedding ETLX in Go applications for seamless integration.</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/ddb-at-the-core/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">database</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">DuckDB at the Core</p><p class="para card-text mb-0">SQL-first transformations and in-process analytics powered by DuckDB</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/multi-engine-execution/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">settings</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Multi-Engine Execution</p><p class="para card-text mb-0">Run ETLX pipelines across DuckDB, PostgreSQL, SQLite, MySQL, SQL Server, and any database supported â€¦</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/beyond-etl/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">layers</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Beyond ETL / ELT</p><p class="para card-text mb-0">ETLX as a declarative specification for reporting, document generation, exports, and regulatory â€¦</p></div></div></a></div><div id=list-item class="col-md-4 col-12 mt-4 pt-2"><a class="text-decoration-none text-reset" href=https://realdatadriven.github.io/etlxdocs/docs/features/api/><div class="card h-100 features feature-full-bg rounded p-4 position-relative overflow-hidden border-1"><span class="icon-color d-flex my-3"><i class="material-icons align-middle">integration_instructions</i></span><div class="card-body p-0 content"><p class="fs-5 fw-semibold card-title mb-1">Go API & Programmatic Usage</p><p class="para card-text mb-0">Use ETLX as a Go library to execute pipelines, run specific stages, or embed ETLX into your own â€¦</p></div></div></a></div></div></div><div><hr class=doc-hr><div id=doc-nav class=d-print-none></div></div></div></div></div></div><footer class="shadow py-3 d-print-none"><div class=container-fluid><div class="row align-items-center"><div class=col><div class="text-sm-start text-center mx-md-2"><p class=mb-0>Â© 2026 Real Data-Driven. Built with <a href=https://github.com/colinwilson/lotusdocs><strong>Lotus Docs</strong></a></p></div></div></div></div></footer></main></div></div><button onclick=topFunction() id=back-to-top aria-label="Back to Top Button" class="back-to-top fs-5"><svg width="24" height="24"><path d="M12 10.224l-6.3 6.3-1.38-1.372L12 7.472l7.68 7.68-1.38 1.376z" style="fill:#fff"/></svg></button>
<script>(()=>{var e=document.getElementById("mode");e!==null&&(window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",e=>{e.matches?(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")):(localStorage.setItem("theme","light"),document.documentElement.removeAttribute("data-dark-mode"))}),e.addEventListener("click",()=>{document.documentElement.toggleAttribute("data-dark-mode"),localStorage.setItem("theme",document.documentElement.hasAttribute("data-dark-mode")?"dark":"light")}),localStorage.getItem("theme")==="dark"?document.documentElement.setAttribute("data-dark-mode",""):document.documentElement.removeAttribute("data-dark-mode"))})()</script><script src=/etlxdocs/docs/js/bootstrap.eac7ee3f6fa791c684e7a51fc2fd50b6f724271b9e39562e4fe8c3942412df3acbfc4045f043d03399dac70091207507.js integrity=sha384-6sfuP2+nkcaE56Ufwv1QtvckJxueOVYuT+jDlCQS3zrL/EBF8EPQM5naxwCRIHUH defer></script><script type=text/javascript src=https://realdatadriven.github.io/etlxdocs/docs/js/bundle.min.3133a96a549adc9fd1c741a7bdae3b9de9f0b398d479269698f0d892e625c24c7422e753e7f25a9c4b6d04f35385f736.js integrity=sha384-MTOpalSa3J/Rx0Gnva47nenws5jUeSaWmPDYkuYlwkx0IudT5/JanEttBPNThfc2 crossorigin=anonymous defer></script><script type=module>
    var suggestions = document.getElementById('suggestions');
    var search = document.getElementById('flexsearch');

    const flexsearchContainer = document.getElementById('FlexSearchCollapse');

    const hideFlexsearchBtn = document.getElementById('hideFlexsearch');

    const configObject = { toggle: false }
    const flexsearchContainerCollapse = new Collapse(flexsearchContainer, configObject) 

    if (search !== null) {
        document.addEventListener('keydown', inputFocus);
        flexsearchContainer.addEventListener('shown.bs.collapse', function () {
            search.focus();
        });
        
        var topHeader = document.getElementById("top-header");
        document.addEventListener('click', function(elem) {
            if (!flexsearchContainer.contains(elem.target) && !topHeader.contains(elem.target))
                flexsearchContainerCollapse.hide();
        });
    }

    hideFlexsearchBtn.addEventListener('click', () =>{
        flexsearchContainerCollapse.hide()
    })

    function inputFocus(e) {
        if (e.ctrlKey && e.key === '/') {
            e.preventDefault();
            flexsearchContainerCollapse.toggle();
        }
        if (e.key === 'Escape' ) {
            search.blur();
            
            flexsearchContainerCollapse.hide();
        }
    };

    document.addEventListener('click', function(event) {

    var isClickInsideElement = suggestions.contains(event.target);

    if (!isClickInsideElement) {
        suggestions.classList.add('d-none');
    }

    });

    


    document.addEventListener('keydown',suggestionFocus);

    function suggestionFocus(e) {
    const suggestionsHidden = suggestions.classList.contains('d-none');
    if (suggestionsHidden) return;

    const focusableSuggestions= [...suggestions.querySelectorAll('a')];
    if (focusableSuggestions.length === 0) return;

    const index = focusableSuggestions.indexOf(document.activeElement);

    if (e.key === "ArrowUp") {
        e.preventDefault();
        const nextIndex = index > 0 ? index - 1 : 0;
        focusableSuggestions[nextIndex].focus();
    }
    else if (e.key === "ArrowDown") {
        e.preventDefault();
        const nextIndex= index + 1 < focusableSuggestions.length ? index + 1 : index;
        focusableSuggestions[nextIndex].focus();
    }

    }

    


    (function(){

    var index = new FlexSearch.Document({
        
        tokenize: "forward",
        minlength:  0 ,
        cache:  100 ,
        optimize:  true ,
        document: {
        id: 'id',
        store: [
            "href", "title", "description"
        ],
        index: ["title", "description", "content"]
        }
    });


    


    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    


    

    

    index.add(
            {
                id:  0 ,
                href: "\/etlxdocs\/docs\/",
                title: "Documentation",
                description: "Documentation for ETLX, a declarative and metadata-first ETL framework for building transparent, reproducible, and observable data pipelines.",
                content: ""
            }
        );
    index.add(
            {
                id:  1 ,
                href: "\/etlxdocs\/docs\/overview\/",
                title: "Overview",
                description: "ETLX is a modern, composable, metadata-driven ETL framework for data engineers.",
                content: "Welcome to the ETLX documentation.\nThis guide explains what ETLX is, why it exists, and how to think about it before diving into pipelines, configuration, and execution details.\nIf you are looking for a fast way to get started, jump directly to the Quickstart.\nWhat is ETLX? linkETLX is an open-source, developer-first ETL framework designed to make data pipelines:\nSimpler More transparent Easier to reason about Fully observable and reproducible Instead of relying on heavyweight orchestration platforms or opaque runtime behavior, ETLX embraces a declarative, metadata-first approach.\nAt its core, ETLX lets you define what should happen â€” and then executes it deterministically, while capturing rich execution metadata along the way.\nWhy ETLX? linkModern data stacks are powerful â€” but often overengineered.\nCommon pain points ETLX aims to solve:\nPipelines that are hard to debug Logic scattered across code, configs, and orchestration tools Poor visibility into what actually happened during execution Documentation that drifts away from reality Vendor lock-in and engine-specific implementations ETLX takes a different path.\nETLX is built on these principles: link Code-first, configuration-driven Database-centric, powered by DuckDB Composable pipelines, not monolithic DAGs Local-first, but production-ready Metadata as a first-class citizen Core Capabilities link schema Declarative Pipelines\nDefine what should happen, not how. Pipelines are structured, readable, and reproducible. database DuckDB at the Core\nSQL-first transformations and in-process analytics powered by DuckDB. hub Multi-Engine Execution\nRun pipelines on DuckDB, PostgreSQL, SQLite, MySQL, SQL Server, and ODBC sources. visibility Full Observability\nEvery execution captures timings, validations, warnings, memory usage, and failure context. "
            }
        );
    index.add(
            {
                id:  2 ,
                href: "\/etlxdocs\/docs\/quickstart\/",
                title: "Quickstart",
                description: "Get ETLX running in minutes and execute your first declarative, metadata-driven data pipeline.",
                content: "âœ… Requirements linkDepending on how you install ETLX:\nMinimum link Linux, macOS, or Windows DuckDB-compatible environment Optional (for building from source) link Go â‰¥ 1.21 git ðŸ“¦ Installation linkChoose one of the following options.\nOption 1: Precompiled Binary (Recommended) linkDownload the latest release for your OS from:\nðŸ‘‰ https://github.com/realdatadriven/etlx/releases\nMake it executable and verify:\nchmod +x etlx ./etlx --help ðŸªŸ Windows \u0026 DuckDB Extensions (Important Note) linkSome DuckDB extensions do not support MinGW on Windows. For this reason, ETLX provides two Windows binaries:\nStatically linked DuckDB (default) Dynamically linked DuckDB (recommended when using more extensions like postgres) If you download the dynamically linked ETLX binary:\nYou must also download libduckdb from the official DuckDB releases: ðŸ‘‰ https://github.com/duckdb/duckdb/releases/latest\nThe libduckdb library must be:\nIn your system PATH, or In the same directory as the etlx binary Otherwise, ETLX will not be able to load DuckDB or its extensions.\nðŸ’¡ This approach allows ETLX to support a wider set of DuckDB extensions on Windows, while keeping the runtime flexible and lightweight.\nWhen should I use the dynamic DuckDB binary? linkUse the dynamic DuckDB build if you:\nAre on Windows Rely on DuckDB extensions not available for MinGW Want closer compatibility with upstream DuckDB releases For Linux and macOS users, the default precompiled binary usually works without additional setup.\nOption 2: Install via Go linkIf you want ETLX as a Go dependency or to build it yourself:\ngo get github.com/realdatadriven/etlx Option 3: Clone the Repository link git clone https://github.com/realdatadriven/etlx.git cd etlx Run directly:\ngo run cmd/main.go --config pipeline.md âš ï¸ Windows note If you encounter DuckDB build issues, use the official DuckDB library and build with:\nCGO_ENABLED=1 CGO_LDFLAGS=\"-L/path/to/libs\" \\ go run -tags=duckdb_use_lib cmd/main.go --config pipeline.md ðŸ§± Your First Pipeline linkETLX pipelines are defined using structured Markdown.\nCreate a file named pipeline.md:\n# INPUTS ```yaml name: INPUTS description: Extracts data from source and load on target runs_as: ETL active: true ``` ## INPUT_1 ```yaml name: INPUT_1 description: Input 1 from an ODBC Source table: INPUT_1 # Destination Table load_conn: \"duckdb:\" load_before_sql: - \"ATTACH 'ducklake:@DL_DSN_URL' AS DL (DATA_PATH 's3://dl-bucket...')\" - \"ATTACH '@OLTP_DSN_URL' AS PG (TYPE POSTGRES)\" load_sql: load_input_in_dl load_on_err_match_patt: '(?i)table.+with.+name.+(\\w+).+does.+not.+exist' load_on_err_match_sql: create_input_in_dl load_after_sql: - DETACH DL - DETACH pg active: true ``` ```sql -- load_input_in_dl INSERT INTO DL.INPUT_1 BY NAME SELECT * FROM PG.INPUT_1 ``` ```sql -- create_input_in_dl CREATE TABLE DL.INPUT_1 AS SELECT * FROM PG.INPUT_1 ``` ... â–¶ï¸ Run the Pipeline link etlx --config pipeline.md Thatâ€™s it.\nETLX will:\nParse the configuration Resolve dependencies Execute steps deterministically Capture execution metadata automatically âš™ï¸ Common CLI Flags link Flag Description --config Path to pipeline file (default: config.md) --date Reference date (YYYY-MM-DD) --only Run only specific keys --skip Skip specific keys --steps Run specific steps (extract, transform, load) --clean Run clean_sql blocks --drop Run drop_sql blocks --rows Show row counts Example:\netlx --config pipeline.md --only sales --steps extract,load ðŸ” Environment Variables linkETLX supports environment-based configuration.\nExample .env file:\nDL_DSN_URL=mysql:db=ducklake_catalog host=localhost OLTP_DSN_URL=postgres:dbname=erpdb host=localhost user=postgres These variables are automatically loaded at runtime.\nðŸ³ Running ETLX with Docker linkYou can run ETLX without installing anything locally.\nBuild the Image link docker build -t etlx:latest . Or pull (when available):\ndocker pull docker.io/realdatadriven/etlx:latest Run a Pipeline link docker run --rm \\ -v $(pwd)/pipeline.md:/app/pipeline.md:ro \\ etlx:latest --config /app/pipeline.md Using .env and Database Directory link docker run --rm \\ -v $(pwd)/.env:/app/.env:ro \\ -v $(pwd)/pipeline.md:/app/pipeline.md:ro \\ -v $(pwd)/database:/app/database \\ etlx:latest --config /app/pipeline.md Interactive Mode link docker run -it --rm etlx:latest repl ðŸ’¡ Optional: Docker Alias linkMake Docker feel like a native binary:\nalias etlx=\"docker run --rm -v $(pwd):/app etlx:latest\" Now:\netlx --help etlx --config pipeline.md ðŸ§  Whatâ€™s Next? link ðŸ“˜ Core Concepts â€“ Pipelines, steps, metadata ðŸ” Execution \u0026 Observability â€“ What ETLX records automatically ðŸ§¾ Self-Documenting Pipelines ðŸ§¬ Metadata â†’ Lineage â†’ Governance ðŸ§© Advanced Use Cases \u0026 Examples ðŸ‘‰ Continue with Core Concepts to understand how ETLX works under the hood.\n"
            }
        );
    index.add(
            {
                id:  3 ,
                href: "\/etlxdocs\/docs\/features\/",
                title: "Features",
                description: "A guide to the core features of the ETLX framework.",
                content: ""
            }
        );
    index.add(
            {
                id:  4 ,
                href: "\/etlxdocs\/docs\/features\/etl-elt\/",
                title: "ETL | ELT",
                description: "How ETLX models, executes, and observes ETL and ELT pipelines using declarative, metadata-driven configurations.",
                content: " ETL | ELT in ETLX linkETLX supports both ETL (Extractâ€“Transformâ€“Load) and ELT (Extractâ€“Loadâ€“Transform) execution models.\nRather than enforcing a specific pattern, ETLX lets you declare the intent of the pipeline, and then executes it deterministically based on metadata.\nAt the highest level, an ETL or ELT pipeline is defined by:\nA root block describing the pipeline One or more execution units (inputs, transformations, outputs) Explicit SQL blocks defining the logic Optional error handling and lifecycle hooks Defining an ETL or ELT Pipeline linkA pipeline is declared using a level-one Markdown heading (#) combined with metadata.\nThe execution mode is defined using the runs_as key:\nruns_as: ETL runs_as: ELT Example link # INPUTS ```yaml name: INPUTS description: Extracts data from source and loads it into the target runs_as: ETL active: true ``` ## INPUT_1 ```yaml name: INPUT_1 description: Input 1 from an ODBC source table: INPUT_1 load_conn: \"duckdb:\" load_before_sql: - \"ATTACH 'ducklake:@DL_DSN_URL' AS DL (DATA_PATH 's3://dl-bucket...')\" - \"ATTACH '@OLTP_DSN_URL' AS PG (TYPE POSTGRES)\" load_sql: load_input_in_dl load_on_err_match_patt: '(?i)table.+with.+name.+(\\w+).+does.+not.+exist' load_on_err_match_sql: create_input_in_dl load_after_sql: - DETACH DL - DETACH PG active: true ``` ```sql -- load_input_in_dl INSERT INTO DL.INPUT_1 BY NAME SELECT * FROM PG.INPUT_1 ``` ```sql -- create_input_in_dl CREATE TABLE DL.INPUT_1 AS SELECT * FROM PG.INPUT_1 ``` @DL_DSN_URL (e.g. mysql:db=ducklake_catalog host=your_mysql_host) and @OLTP_DSN_URL (e.g. postgres:dbname=erpdb host=your_postgres_host user=postgres password=your_pass) are environment variables used to define database connection strings. They can be provided through a .env file located at the root of the project and are automatically loaded at runtime. These variables allow ETLX to connect to different data sources without hardcoding credentials, making configurations portable, secure, and environment-agnostic.\nExecution Model link1. Pipeline Initialization linkExecution starts at the root pipeline block (# INPUTS).\nFrom this block, ETLX extracts:\nPipeline metadata\nname description runs_as (ETL or ELT) Global execution context\nDefault connection Execution timestamp Runtime metadata Activation state\nPipelines marked as active: false are skipped This metadata becomes part of the execution trace and observability layer.\n2. Iteration Over Execution Units linkEach level-two heading (## INPUT_1, ## TRANSFORM_X, etc.) represents an execution unit.\nFor each unit, ETLX:\nReads its metadata Resolves connections Determines which steps apply Executes steps in a deterministic order Inactive units (active: false) are skipped but still recorded in metadata.\nETL / ELT Steps linkEach execution unit may define one or more steps, depending on the execution model:\nextract transform load Each step supports a consistent lifecycle:\nStep Lifecycle Hooks linkFor any step (e.g. load):\nHook Purpose _before_sql Setup (e.g. attach databases, prepare schemas) _sql Main execution logic _after_sql Cleanup (detach, finalize, release resources) SQL Resolution Rules linkEach SQL hook can be defined as:\nnull â†’ step is skipped string â†’ reference to a named SQL block inline SQL string list/array â†’ executed sequentially Named SQL blocks are resolved from:\nsql [query_name] blocks Or SQL comments: -- query_name This allows clear separation of metadata and logic.\nConnection Handling linkEach step can specify a connection using _conn.\nIf defined â†’ used for that step If null or omitted â†’ falls back to the pipelineâ€™s default connection This enables multi-engine pipelines (DuckDB, Postgres, ODBC, etc.) within a single execution.\nError Handling \u0026 Recovery linkETLX provides pattern-based error handling, allowing pipelines to recover dynamically from known failure conditions.\nFor any step :\n_on_err_match_patt A regular expression matched against the database error message _on_err_match_sql SQL executed when the pattern matches This is especially useful for:\nCreating missing tables Initializing schemas Handling first-run scenarios Working with evolving datasets The same mechanism applies to:\n_before_on_err_match_* Observability \u0026 Execution Metadata linkEvery ETLX run is fully observable by design.\nFor each pipeline, step, and sub-step, ETLX records:\nStart and end timestamps Execution duration Connection used SQL executed Rows affected (when available) Errors, warnings, and retries Memory and resource usage This metadata can be:\nLogged Stored Queried via SQL Used to generate reports and documentation Design Principles linkETLX treats ETL and ELT pipelines as:\nDeclarative execution plans Structured metadata documents Executable documentation This ensures pipelines are:\nReproducible Inspectable Auditable Portable across environments Easy to maintain and evolve Summary linkIn ETLX:\nETL and ELT are modes, not rigid architectures Configuration defines intent, not orchestration SQL remains first-class Metadata powers execution, observability, and documentation Pipelines are self-describing and deterministic Your pipeline configuration is your source of truth.\n"
            }
        );
    index.add(
            {
                id:  5 ,
                href: "\/etlxdocs\/docs\/features\/validation\/",
                title: "ETL | ELT - Validation Rules",
                description: "How ETLX enables declarative data quality validation as a first-class concern of ETL / ELT execution blocks.",
                content: " Validation Rules linkETLX allows data quality validation to be declared as metadata, making validation a first-class concern of ETL / ELT execution.\nValidations are executed as part of the pipeline lifecycle, not as an external check or post-process.\nThey allow you to:\nFail fast on invalid data Guard against silent data corruption Encode business and technical expectations declaratively Keep validation logic close to the data movement it protects Declaring Validations linkValidations are defined using the metadata key:\n_validation Where can be any supported execution step, such as:\nextract transform load Example linkIn the previous example from etl/elt, we define two validation rules for the load step:\n...\r```yaml\r...\rload_validation:\r- type: throw_if_empty\rsql: validate_data_not_empty\rmsg: \"The given date is not avaliable in the origin!\"\ractive: true\r- type: throw_if_not_empty\rsql: validate_date_exists_in_destination\rmsg: \"This date is already imported, check to avoid duplicates!\"\ractive: true\r...\r```\r```sql validate_data_not_empty\rSELECT *\rFROM PG.INPUT_1\rWHERE event_date = '{YYYY-MM-DD}'\rLIMIT 10\r```\r```sql validate_date_exists_in_destination\rSELECT *\rFROM DL.INPUT_1 WHERE event_date = '{YYYY-MM-DD}'\rLIMIT 10\r``` Each validation rule is evaluated after the step SQL executes and before the next lifecycle hook.\nhere we used {YYYY-MM-DD} as a placeholder for date, which ETLX will replace at runtime, if cli used with --date argument, that date would be the one placede in {YYYY-MM-DD}.\nValidation Execution Model linkFor each object defined in _validation:\nThe associated sql is executed\nThe query result is evaluated according to the type\nIf the condition is met, ETLX:\nThrows a controlled execution error Emits the configured message (msg) Marks the step as failed in execution metadata Validation Connection Resolution linkValidation SQL is executed using:\n_conn, if defined Otherwise, the pipelineâ€™s default connection This ensures validations run in the same execution context as the data they validate.\nSupported Validation Types link Type Behavior throw_if_empty Fails if the query returns zero rows throw_if_not_empty Fails if the query returns one or more rows Validation types are intentionally simple and composable. Complex rules should be expressed in SQL, not in configuration logic. This keeps validation declarative and database-native.\nSQL Validation Design linkValidation SQL should express intent, not mechanics.\nTypical validation queries include:\nRow existence checks Duplicate detection This is mostly usefull to avolid data duplication or missing data during incremental loads, or if needed to run the extraction more than once because of some input failure, without compromising inputs that were already extracted.\nValidation Observability linkEach validation execution is recorded as part of ETLXâ€™s observability layer:\nValidation name Step and pipeline association SQL executed Execution time Result (pass / fail) Error message (if thrown) This enables:\nAuditing SLA enforcement Debugging Reporting Design Rationale linkETLX treats validation as:\nExecutable documentation A contract between pipeline stages A guardrail, not a sidecar By encoding validation rules in metadata:\nPipelines become safer by default Failures are explicit and explainable Validation logic evolves with the pipeline Key Takeaways link Validation is declarative, explicit, and observable SQL remains the single source of truth Pipelines fail loudly, not silently Unsupported sources do not block adoption ETLX prioritizes correctness over convenience "
            }
        );
    index.add(
            {
                id:  6 ,
                href: "\/etlxdocs\/docs\/features\/query-documentation\/",
                title: "Query Documentation",
                description: "How ETLX models SQL queries as executable documentation to improve clarity, governance, and reusability.",
                content: "Query Documentation linkIn ETLX, SQL queries are not treated as opaque strings. Instead, they are modeled as structured, inspectable, and executable documents.\nThis is particularly important in ETL and ELT pipelines where transformation logic tends to grow organically and quickly becomes difficult to understand, govern, and audit. Query Documentation addresses this problem by allowing SQL logic to be decomposed into field-level components enriched with metadata.\nThe result is not just better readability, but a foundation for data governance, lineage, and automated documentation.\nWhy Query Documentation Exists in ETLX linkTraditional ETL systems treat SQL as an implementation detail. ETLX treats it as knowledge.\nBy documenting queries as structured metadata, ETLX enables:\nTransparent and explainable transformations Column-level data dictionaries Source-to-target lineage Ownership and accountability Auditability and reproducibility Query Documentation is therefore not a formatting convenience â€” it is a core modeling primitive in ETLX.\nMental Model: QueryDoc linkA QueryDoc is a first-class ETLX object that represents a documented SQL query.\nA QueryDoc:\nHas a unique name Contains descriptive metadata (owner, description, source, etc.) Is composed of documented fields Resolves deterministically into executable SQL Can be referenced and reused across pipelines At runtime, ETLX assembles the QueryDoc into a valid SQL statement, records the resolved SQL, and executes it as part of the pipeline.\nStructure linkA documented query is defined as a level-one Markdown heading (# QUERY_NAME).\nEach field participating in the query is represented as a level-two heading (## field_name).\nFor each field:\nOptional YAML metadata can describe the field (description, type, owner, derived_from, formula, etc.)\nSQL components are split into separate SQL blocks such as:\nselect from join where group_by having order_by cte Instead of maintaining a single large SQL string, ETLX assembles these components at runtime in a deterministic order.\nThis approach requires intentional design, but pays off by making transformations self-documenting and governable.\nExample: Documenting an Extraction Query linkThe following example shows how ETLX can be extended with metadata keys useful for governance when extracting data from an external source.\nThe example uses the NYC Yellow Taxi January 2024 Parquet dataset hosted at:\nhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n# EXTRACT_LOAD\r```yaml metadata\rname: EXTRACT_LOAD\rruns_as: ETL\rdescription: |\rExtracts and Loads the data sets to the local analitical database\rconnection: \"sqlite3:database/DB_EX_DGOV.db\"\rdatabase: \"sqlite3:database/DB_EX_DGOV.db\"\ractive: true\r```\r## TRIP_DATA\r```yaml metadata\rname: TRIP_DATA\rdescription: \"Example extrating trip data from web to a local database\"\rtable: TRIP_DATA\rload_conn: \"duckdb:\"\rload_before_sql: \"ATTACH 'database/DB_EX_DGOV.db' AS DB (TYPE SQLITE)\"\rload_sql: extract_load_trip_data\rload_after_sql: DETACH \"DB\"\ractive: false\r```\r```sql\r-- extract_load_trip_data\rCREATE OR REPLACE TABLE \"DB\".\"\" AS\rFROM read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet')\r```\r... Replacing Inline SQL with a QueryDoc linkIf your organization requires a standardized data dictionary for every input â€” or if the same extraction logic is reused across multiple pipelines â€” the query can be externalized into a QueryDoc.\n# EXTRACT_LOAD\r...\r```sql\r-- extract_load_trip_data\rCREATE OR REPLACE TABLE \"DB\".\"\" AS\r[[QUERY_EXTRACT_TRIP_DATA]]\r``` The placeholder [[QUERY_EXTRACT_TRIP_DATA]] references a documented query defined elsewhere in the configuration.\nQuery Resolution Semantics linkQuery placeholders are resolved before execution.\nETLX guarantees that:\nAll referenced QueryDocs must exist SQL components are assembled in a deterministic order Missing or incompatible components fail fast The fully resolved SQL is logged and observable This ensures that documented queries remain safe, predictable, and auditable.\nDocumenting Fields with Metadata linkâ€¦\n# QUERY_EXTRACT_TRIP_DATA\rThis QueryDoc extracts selected fields from the NYC Yellow Taxi Trip Record dataset.\r```yaml metadata\rname: QUERY_EXTRACT_TRIP_DATA\rdescription: \"Extracts essential NYC Yellow Taxi trip fields (with governance metadata).\"\rowner: taxi-analytics-team\rdetails: \"https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\"\rsource:\ruri: \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\rformat: parquet\r```\r## VendorID\r```yaml metadata\rname: VendorID\rdescription: \"A code indicating which TPEP provider generated the record.\r1=CMT, 2=Curb, 6=Myle, 7=Helix.\"\rtype: integer\rowner: data-providers\r```\r```sql\r-- select\rSELECT VendorID\r```\r```sql\r-- from\rFROM read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet')\r```\r## tpep_pickup_datetime\r```yaml metadata\rname: tpep_pickup_datetime\rdescription: \"Timestamp when the meter was engaged (trip start).\"\rtype: timestamp\rowner: taxi-analytics-team\r```\r```sql\r-- select\r, tpep_pickup_datetime\r```\r...\r## RatecodeID\r```yaml metadata\rname: RatecodeID\rdescription: \"Final rate code at trip end.\r1=Standard, 2=JFK, 3=Newark, 4=Nassau/Westchester,\r5=Negotiated fare, 6=Group ride, 99=Unknown.\"\rtype: integer\rowner: finance\r```\r...\r## payment_type\r```yaml metadata\rname: payment_type\rdescription: \"How the passenger paid:\r0=Flex Fare, 1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided trip.\"\rtype: integer\rowner: finance\r```\r... This approach allows ETLX to generate or support:\nColumn-level data dictionaries Field ownership maps Source attribution Lineage inputs for external tools Documenting Transformation Queries linkQuery Documentation becomes even more valuable when applied to derived and calculated fields, where logic is owned by the organization and rarely documented elsewhere.\nThe following example documents a transformation that identifies the most popular taxi routes.\n...\r# TRANSFORM\r```yaml metadata\rname: TRANSFORM\rruns_as: ETL\rdescription: Transforms the inputs into to desrable outputs\rconnection: \"sqlite3:database/DB_EX_DGOV.db\"\rdatabase: \"sqlite3:database/DB_EX_DGOV.db\"\ractive: true\r```\r## MostPopularRoutes\r```yaml metadata\rname: MostPopularRoutes\rdescription: |\rMost Popular Routes - Identify the most common pickup-dropoff route combinations to understand travel patterns.\rtable: MostPopularRoutes\rtransform_conn: \"duckdb:\"\rtransform_before_sql: \"ATTACH 'database/DB_EX_DGOV.db' AS DB (TYPE SQLITE)\"\rtransform_sql: trf_most_popular_routes\rtransform_after_sql: DETACH \"DB\"\rdrop_sql: DROP TABLE IF EXISTS \"DB\".\"\"\rclean_sql: DELETE FROM \"DB\".\"\"\rrows_sql: SELECT COUNT(*) AS \"nrows\" FROM \"DB\".\"\"\ractive: true\r```\r```sql\r-- trf_most_popular_routes\rCREATE OR REPLACE TABLE \"DB\".\"\" AS\r[[QUERY_TOP_ZONES]]\rLIMIT 15\r```\r# QUERY_TOP_ZONES\r```yaml metadata\rname: QUERY_TOP_ZONES\rdescription: \"Most common pickup/dropoff zone combinations with aggregated metrics.\"\rowner: taxi-analytics-team\rsource:\r- TRIP_DATA\r- ZONES\r```\r## pickup_borough\r```yaml metadata\rname: pickup_borough\rdescription: \"Borough of the pickup location (from ZONES lookup).\"\rtype: string\rderived_from:\r- TRIP_DATA.PULocationID\r- ZONES.Borough\rformula: |\rThis field is a direct lookup of the pickup borough using the pickup location ID.\r```\r```sql\r-- select\rSELECT zpu.Borough AS pickup_borough\r```\r```sql\r-- from\rFROM DB.TRIP_DATA AS t\r```\r```sql\r-- join\rJOIN DB.ZONES AS zpu ON t.PULocationID = zpu.LocationID\r```\r```sql\r-- group_by\rGROUP BY pickup_borough\r```\r## pickup_zone\r```yaml metadata\rname: pickup_zone\rdescription: \"Zone name of the pickup location (from ZONES lookup).\"\rtype: string\rderived_from:\r- TRIP_DATA.PULocationID\r- ZONES.Zone\rformula: |\rThis field is a direct lookup of the pickup zone using the pickup location ID.\r```\r```sql\r-- select\r, zpu.Zone AS pickup_zone\r```\r```sql\r-- group_by\r, pickup_zone\r```\r## dropoff_borough\r```yaml metadata\rname: dropoff_borough\rdescription: \"Borough of the dropoff location (from ZONES lookup).\"\rtype: string\rderived_from:\r- TRIP_DATA.DOLocationID\r- ZONES.Borough\rformula: |\rThis field is a direct lookup of the dropoff borough using the dropoff location ID.\r```\r```sql\r-- select\r, zdo.Borough AS dropoff_borough\r```\r```sql\r-- join\rJOIN DB.ZONES AS zdo ON t.DOLocationID = zdo.LocationID\r```\r```sql\r-- group_by\r, dropoff_borough\r```\r## dropoff_zone\r```yaml metadata\rname: dropoff_zone\rdescription: \"Zone name of the dropoff location (from ZONES lookup).\"\rtype: string\rderived_from:\r- TRIP_DATA.DOLocationID\r- ZONES.Zone\rformula: |\rThis field is a direct lookup of the dropoff zone using the dropoff location ID.\r```\r```sql\r-- select\r, zdo.Zone AS dropoff_zone\r```\r```sql\r-- group_by\r, dropoff_zone\r```\r## total_trips\r```yaml metadata\rname: total_trips\rdescription: \"Total number of trips between each pickup/dropoff zone pair.\"\rtype: integer\rderived_from:\r- TRIP_DATA.*\rformula: | \"\rCount the number of trips in each pickup/dropoff combination.\r**Formula (LaTeX):**\r```latex\r\\text{total\\_trips} = \\sum_{i=1}^{N} 1\r```\"\r```\r```sql\r-- select\r, COUNT(*) AS total_trips\r```\r```sql\r-- order_by\rORDER BY total_trips DESC\r```\r## avg_fare\r```yaml metadata\rname: avg_fare\rdescription: \"Average total fare for trips between the selected pickup and dropoff zones.\"\rtype: numeric\rderived_from:\r- TRIP_DATA.total_amount\rformula: | \"\rCompute the arithmetic mean of total fares for the group, rounded to 2 decimals.\r**Formula (LaTeX):**\r```latex\r\\text{avg\\_fare} = \\text{round}\\!\\left(\\frac{1}{N}\\sum_{i=1}^{N} \\text{total\\_amount}_i,\\ 2\\right)\r```\"\r```\r```sql\r-- select\r, ROUND(AVG(t.total_amount), 2) AS avg_fare\r```\r## avg_distance\r```yaml metadata\rname: avg_distance\rdescription: \"Average trip distance (miles).\"\rtype: numeric\rderived_from:\r- TRIP_DATA.trip_distance\rformula: | \"\rCompute the arithmetic mean of trip distances for the group, rounded to 2 decimals.\r**Formula (LaTeX):**\r```latex\r\\text{avg\\_distance} = \\text{round}\\!\\left(\\frac{1}{N}\\sum_{i=1}^{N} \\text{trip\\_distance}_i,\\ 2\\right)\r```\"\r```\r```sql\r-- select\r, ROUND(AVG(t.trip_distance), 2) AS avg_distance\r```\r... Each derived field documents:\nBusiness meaning Source fields Transformation logic Ownership This turns transformation SQL into executable business documentation.\nWhat This Is â€” and Is Not linkQuery Documentation in ETLX is:\nA structured SQL modeling approach A metadata foundation for governance A bridge between execution and documentation It is not:\nA replacement for SQL A data catalog A BI semantic layer Instead, ETLX provides high-quality metadata that can feed those systems.\nSummary linkIn ETLX:\nSQL expresses logic Metadata provides context Together they form executable knowledge Query Documentation ensures that complex queries remain understandable, auditable, and reusable â€” without sacrificing the power and flexibility of SQL.\nYour pipeline configuration becomes not just runnable code, but a durable source of truth.\nThis documentation itself can be used to generate governance artifatcts based on your own template and format like shown in this example Metadata \u0026 Governance Doc\n"
            }
        );
    index.add(
            {
                id:  7 ,
                href: "\/etlxdocs\/docs\/features\/data-quality\/",
                title: "Data Quality",
                description: "How ETLX models, executes, and observes data quality rules as first-class, metadata-driven validation processes.",
                content: "Data Quality in ETLX linkData quality in ETLX is not an afterthought or an external process. It is a first-class execution model, designed to validate, observe, and optionally correct data using declarative, SQL-driven rules.\nRather than embedding ad-hoc checks inside transformations, ETLX treats data quality as:\nExecutable metadata Auditable validation logic A governance primitive This allows quality rules to be reasoned about, documented, versioned, and executed independently of ETL or ELT pipelines.\nThe DATA_QUALITY Execution Model linkA data quality workflow is defined using a root Markdown block that declares its execution mode as DATA_QUALITY.\nFrom this block, ETLX executes a deterministic sequence of validation rules, each modeled as an independent execution unit.\nConceptually, a DATA_QUALITY block answers three questions:\nWhat condition defines invalid data? How should violations be observed and reported? Can (and should) invalid data be fixed automatically? Structure Overview linkA DATA_QUALITY definition consists of:\nRoot block metadata\nDescribes the purpose and activation state of the quality process Validation rules\nEach rule is a level-two heading representing a single, focused quality check SQL logic\nOne query to detect violations An optional query to correct them Data Quality Markdown Example linkThe example below illustrates a DATA_QUALITY workflow validating the TRIP_DATA dataset introduced in the Query Documentation section (NYC Yellow Taxi data).\n# QUALITY_CHECK\r```yaml\rdescription: \"Runs some queries to check quality / validate.\"\rruns_as: DATA_QUALITY\ractive: true\r```\r## Rule0001\r```yaml\rname: Rule0001\rdescription: \"Check if the payment_type has only the values 0=Flex Fare, 1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided trip.\"\rconnection: \"duckdb:\"\rbefore_sql: \"ATTACH 'database/DB_EX_DGOV.db' AS DB (TYPE SQLITE)\"\rquery: quality_check_query\rfix_quality_err: fix_quality_err_query\rcolumn: total_err # Defaults to 'total'.\rcheck_only: false # runs only quality check if true\rfix_only: false # runs only quality fix if true and available and possible\rafter_sql: \"DETACH DB\"\ractive: true\r```\r```sql\r-- quality_check_query\rSELECT COUNT(*) AS \"total_err\"\rFROM \"TRIP_DATA\"\rWHERE \"payment_type\" NOT IN (0,1,2,3,4,5,6);\r```\r```sql\r-- fix_quality_err_query\rUPDATE \"TRIP_DATA\"\rSET \"payment_type\" = 'default value'\rWHERE \"payment_type\" NOT IN (0,1,2,3,4,5,6);\r```\r## Rule0002\r```yaml\rname: Rule0002\rdescription: \"Check if there is any trip with distance less than or equal to zero.\"\rconnection: \"duckdb:\"\rbefore_sql: \"ATTACH 'database/DB_EX_DGOV.db' AS DB (TYPE SQLITE)\"\rquery: quality_check_query\rfix_quality_err: null # no automated fixing for this\rcolumn: total_err # Defaults to 'total'.\rafter_sql: \"DETACH DB\"\ractive: true\r```\r```sql\r-- quality_check_query\rSELECT COUNT(*) AS \"total_err\"\rFROM \"TRIP_DATA\"\rWHERE NOT \"trip_distance\" \u003e 0;\r``` Rule Semantics linkEach validation rule represents a contract that the data must satisfy.\nA rule is defined by:\nDetection logic (query) Optional remediation logic (fix_quality_err) Execution controls (check_only, fix_only) Lifecycle hooks (before_sql, after_sql) Rules are intentionally small and composable, encouraging one responsibility per rule.\nExecution Flow linkFor each active rule, ETLX executes the following steps:\nInitialization\nResolve connection Execute before_sql Validation\nExecute the detection query Read the violation count from the configured column Decision\nIf violations = 0 â†’ rule passes If violations \u003e 0 â†’ rule fails Optional Fix\nIf fix_quality_err is defined and allowed Execute remediation SQL Finalization\nExecute after_sql Record execution metadata This flow is deterministic and fully observable.\nQuery \u0026 Connection Resolution link Validation queries run using the ruleâ€™s connection\nIf omitted, the DATA_QUALITY root connection is used\nQueries can reference:\nInline SQL Named SQL blocks Shared datasets such as TRIP_DATA This allows quality rules to be applied consistently across:\nExtracted data Transformed outputs Final reporting tables Observability \u0026 Governance linkEvery DATA_QUALITY execution produces structured metadata, including:\nRule name and description Execution timestamps and duration Number of violations detected Fix actions applied Errors and warnings This metadata can be used to:\nGenerate data quality reports Track SLA compliance Power dashboards Feed governance and audit systems Design Principles linkETLX data quality rules are:\nDeclarative â€” define intent, not control flow SQL-native â€” no DSLs, no abstractions hiding logic Deterministic â€” same input, same result Auditable â€” every action is recorded Composable â€” reusable across pipelines What This Is (and Is Not) linkThis is:\nA data quality execution model A governance-friendly validation layer An observable contract enforcement mechanism This is not:\nA black-box data quality tool A replacement for BI validation A rule engine detached from SQL ETLX complements catalogs, observability platforms, and BI tools by enforcing quality at the source of execution.\nSummary linkIn ETLX:\nData quality is code, not comments Rules are first-class execution units SQL defines truth Metadata defines meaning Data quality is not something you check once â€” it is something you continuously execute, observe, and govern.\n"
            }
        );
    index.add(
            {
                id:  8 ,
                href: "\/etlxdocs\/docs\/features\/multi-query\/",
                title: "Multi / Stacked Queries",
                description: "Combine multiple structured queries into a single result set using UNION-based execution.",
                content: "Multi / Stacked Queries linkThe MULTI_QUERIES block allows you to define multiple independent but structurally compatible queries and combine their results into a single unified dataset using SQL UNION or UNION ALL. This pattern is especially useful for reporting, KPI generation, metric tables, and summary datasets.\nInstead of orchestrating many individual queries manually, MULTI_QUERIES lets you declaratively describe each row or slice of a report and automatically aggregate them into a single execution plan.\nMulti Queries Structure link1. Metadata linkThe Levelâ€‘1 MULTI_QUERIES block defines shared execution metadata:\nConnection and lifecycle SQL (before_sql, after_sql) How queries are merged (union_key) Optional persistence logic (save_sql) Errorâ€‘aware save handling (save_on_err_patt, save_on_err_sql) This metadata applies to all child queries unless explicitly overridden.\n2. Query Definitions link Each Levelâ€‘2 heading represents a single query fragment. All queries must return compatible schemas (same column names and types). Each query contributes one or more logical rows to the final result. 3. Execution Model link All active queries are collected. Queries are concatenated using the defined union_key (default: UNION). The final SQL is executed once, optionally persisted. Multi Queries Markdown Example link # SALES_REPORT\r```yaml\rdescription: \"Define multiple structured queries combined with UNION.\"\rruns_on: MULTI_QUERIES\rconnection: \"duckdb:\"\rbefore_sql: \"ATTACH 'reporting.db' AS DB (TYPE SQLITE)\"\rsave_sql: save_mult_query_res\rsave_on_err_patt: '(?i)table.+with.+name.+(\\w+).+does.+not.+exist'\rsave_on_err_sql: create_mult_query_res\rafter_sql: \"DETACH DB\"\runion_key: \"UNION ALL\\n\" # Defaults to UNION\ractive: true\r```\r```sql\r-- save_mult_query_res\rINSERT INTO \"DB\".\"MULTI_QUERY\" BY NAME\r[[final_query]]\r```\r```sql\r-- create_mult_query_res\rCREATE OR REPLACE TABLE \"DB\".\"MULTI_QUERY\" AS\r[[final_query]]\r```\r## Row1\r```yaml\rname: Row1\rdescription: \"Row count\"\rquery: row_query\ractive: true\r```\r```sql\r-- row_query\rSELECT '# number of rows' AS \"variable\", COUNT(*) AS \"value\"\rFROM \"sales\"\r```\r## Row2\r```yaml\rname: Row2\rdescription: \"Total revenue\"\rquery: row_query\ractive: true\r```\r```sql\r-- row_query\rSELECT 'total revenue' AS \"variable\", SUM(\"total\") AS \"value\"\rFROM \"sales\"\r```\r## Row3\r```yaml\rname: Row3\rdescription: \"Revenue by region\"\rquery: row_query\ractive: true\r```\r```sql\r-- row_query\rSELECT \"region\" AS \"variable\", SUM(\"total\") AS \"value\"\rFROM \"sales\"\rGROUP BY \"region\"\r``` How Multi Queries Works link1. Query Collection link All Levelâ€‘2 queries marked as active: true are selected. Each query resolves its SQL either inline or via named SQL blocks. 2. Query Union link Queries are joined using the union_key value:\nUNION removes duplicates UNION ALL preserves all rows 3. Final Query Execution linkThe engine produces a single executable SQL statement:\nSELECT ...\rUNION ALL\rSELECT ...\rUNION ALL\rSELECT ... 4. Optional Persistence linkIf save_sql is defined:\nThe final query is injected using [[final_query]]. If execution fails and the error matches save_on_err_patt, the fallback SQL (save_on_err_sql) is executed. This enables insertâ€‘orâ€‘create patterns for reporting tables.\nExample Final Query (Expanded) link LOAD sqlite;\rATTACH 'reporting.db' AS DB (TYPE SQLITE);\rSELECT '# number of rows' AS \"variable\", COUNT(*) AS \"value\"\rFROM \"sales\"\rUNION ALL\rSELECT 'total revenue' AS \"variable\", SUM(\"total\") AS \"value\"\rFROM \"sales\"\rUNION ALL\rSELECT \"region\" AS \"variable\", SUM(\"total\") AS \"value\"\rFROM \"sales\"\rGROUP BY \"region\";\rDETACH DB; Multi Queries Use Cases link KPI and metric tables Executive dashboards Summary datasets Audit and reconciliation reports Crossâ€‘domain aggregations Multi Queries Benefits link Declarative reporting: define results, not execution order Singleâ€‘pass execution: efficient and optimized Errorâ€‘aware persistence: safe table creation and inserts Highly composable: pairs naturally with ETL and DATA_QUALITY blocks The MULTI_QUERIES block turns SQL reporting into a structured, reusable, and auditable pipeline component, fully aligned with ETLXâ€™s blockâ€‘driven design philosophy.\n"
            }
        );
    index.add(
            {
                id:  9 ,
                href: "\/etlxdocs\/docs\/features\/exports\/",
                title: "Exports",
                description: "Handling data exports to files and templates using ETLX",
                content: " Exports linkThe EXPORTS block defines how ETLX materializes data into external artifacts such as files, reports, and documents. Exports are typically the final stage of a pipeline, where curated data is delivered to downstream consumers like analysts, regulators, partners, or automated systems.\nUnlike ETL or MULTI_QUERIES, which focus on data movement and computation, EXPORTS is concerned with presentation, distribution, and persistence outside the database.\nETLX leverages DuckDBâ€™s native COPY capabilities and extends them with template-based rendering to support both structured and text-based outputs.\nExport Structure linkAn EXPORTS block follows the same declarative pattern used throughout ETLX:\nBlock Metadata\nDefines connection, base path, activation, and shared execution context. Export Units (Level 2 headings)\nEach export represents a single file or document to be produced. Exports may write directly via SQL or populate templates. Execution Lifecycle\nOptional before_sql and after_sql hooks SQL execution and file materialization Example Configuration link # DAYLY_REPORTS\r```yaml metadata\rname: DailyExports\rdescription: \"Daily file exports for various datasets.\"\rruns_as: EXPORTS\rdatabase: reporting_db\rconnection: \"duckdb:\"\rpath: \"/path/to/Reports/YYYYMMDD\"\ractive: true\r```\r## Sales Data Export\r```yaml metadata\rname: SalesExport\rdescription: \"Export daily sales data to CSV.\"\rconnection: \"duckdb:\"\rexport_sql:\r- \"LOAD sqlite\"\r- \"ATTACH 'reporting.db' AS DB (TYPE SQLITE)\"\r- export_query\r- \"DETACH DB\"\ractive: true\r```\r```sql\r-- export_query\rCOPY (\rSELECT *\rFROM \"DB\".\"Sales\"\rWHERE \"sale_date\" = '{YYYY-MM-DD}'\r) TO '/path/to/Reports/YYYYMMDD/sales_YYYYMMDD.csv' (FORMAT 'csv', HEADER true);\r```\r## Region Data Export to Excel\r```yaml metadata\rname: RegionExport\rdescription: \"Export region data to an Excel file.\"\rconnection: \"duckdb:\"\rexport_sql:\r- \"LOAD sqlite\"\r- \"LOAD excel\"\r- \"ATTACH 'reporting.db' AS DB (TYPE SQLITE)\"\r- export\r- \"DETACH DB\"\ractive: true\r```\r```sql\r-- export\rCOPY (\rSELECT *\rFROM \"DB\".\"Regions\"\rWHERE \"updated_at\" \u003e= '{YYYY-MM-DD}'\r) TO '/path/to/Reports/YYYYMMDD/regions_YYYYMMDD.xlsx' (FORMAT XLSX, HEADER TRUE);\r``` Template-Based Exports linkIn addition to raw file exports, ETLX supports template-driven exports. These are ideal for producing standardized reports where query results must be injected into predefined layouts.\nTemplates are commonly used with Excel, but the same concept applies to text-based formats such as HTML or Markdown.\nExcel Template Example link ## Sales Report Template\r```yaml metadata\rname: SalesReport\rdescription: \"Generate a sales report from a template.\"\rconnection: \"duckdb:\"\rbefore_sql:\r- \"LOAD sqlite\"\r- \"ATTACH 'reporting.db' AS DB (TYPE SQLITE)\"\rtemplate: \"/path/to/Templates/sales_template.xlsx\"\rpath: \"/path/to/Reports/sales_report_YYYYMMDD.xlsx\"\rmapping:\r- sheet: Summary\rrange: B2\rsql: summary_query\rtype: range\rtable: SummaryTable\rtable_style: TableStyleLight1\rheader: true\rif_exists: delete\r- sheet: Details\rrange: A1\rsql: details_query\rtype: value\rkey: total_sales\rafter_sql: \"DETACH DB\"\ractive: true\r```\r```sql\r-- summary_query\rSELECT SUM(total_sales) AS total_sales\rFROM \"DB\".\"Sales\"\rWHERE \"sale_date\" = '{YYYY-MM-DD}'\r```\r```sql\r-- details_query\rSELECT *\rFROM \"DB\".\"Sales\"\rWHERE \"sale_date\" = '{YYYY-MM-DD}';\r``` Mapping Notes link mapping can be:\nA YAML list (inline definition) A string referencing a query Loaded dynamically from a database table In real-world scenarios, mappings often grow large and are easier to maintain in spreadsheets or database tables rather than static configuration files.\nText-Based Template Exports linkETLX also supports exporting reports using plain-text templates such as HTML, XML, or Markdown. These exports use Goâ€™s text/template engine and receive query results as structured input data.\nThis mechanism is shared with the NOTIFY block and is well-suited for:\nHTML/XML/Markdown reports Integration payloads Generated documentation Email-ready content Example link ## TEXT_TMPL\r```yaml metadata\rname: TEXT_TMPL\rdescription: \"Export data to text base template\"\rconnection: \"duckdb:\"\rbefore_sql:\r- \"INSTALL sqlite\"\r- \"LOAD sqlite\"\r- \"ATTACH 'database/HTTP_EXTRACT.db' AS DB (TYPE SQLITE)\"\rdata_sql:\r- logs\r- data\rafter_sql: \"DETACH DB\"\rtmp_prefix: null\rtext_template: true\rtemplate: template\rreturn_content: false\rpath: \"nyc_taxy_YYYYMMDD.html\"\ractive: true\r```\r```sql\r-- data\rSELECT *\rFROM \"DB\".\"NYC_TAXI\"\rWHERE \"tpep_pickup_datetime\"::DATETIME \u003c= '{YYYY-MM-DD}'\rLIMIT 100\r```\r```sql\r-- logs\rSELECT *\rFROM \"DB\".\"etlx_logs\"\r--WHERE \"ref\" = '{YYYY-MM-DD}'\r```\r```html template\rETLX Text Template\nThis is gebnerated by ETLX automatically!\n{{ with .logs }}\r{{ if eq .success true }}\rName\rRef\rStart\rEnd\rDuration\rSuccess\rMessage\r{{ range .data }}\r{{ .name }}\r{{ .ref }}\r{{ .start_at | date \"2006-01-02 15:04:05\" }}\r{{ .end_at | date \"2006-01-02 15:04:05\" }}\r{{ divf .duration 1000000000 | printf \"%.4fs\" }}\r{{ .success }}\r{{ .msg | toString | abbrev 30}}\r{{ else }}\rNo items available\r{{ end }}\r{{ else }}\r{{.msg}}\n{{ end }}\r{{ else }}\rLogs information missing.\n{{ end }}\r``` Parameters link Field Description text_template Enables text-based template rendering template SQL block containing the Go template data_sql Named SQL blocks whose results feed the template path Output file path (supports placeholders) return_content If true, returns content instead of writing to disk ðŸ§° Advanced Template Functions (Sprig) linkETLX integrates the Sprig The Sprig library provides over 70 template functions for Goâ€™s template language, such as:\nString manipulation: upper, lower, trim, contains, replace Math: add, mul, round Date formatting: date, now, dateModify List operations: append, uniq, join You can use these helpers directly in your templates:\n{{ .ref | upper }}\r{{ .start_at | date \"2006-01-02\" }} This enables powerful report generation and custom formatting out-of-the-box.\nHow Exports Work link Parse Configuration\nEach export is treated as an independent execution unit. Prepare Environment\nExecutes before_sql hooks and loads required extensions. Materialize Output\nExecutes export SQL or applies template mappings. Finalize\nRuns after_sql hooks and closes resources. Benefits of EXPORTS link Format Flexibility: CSV, Excel, Parquet, HTML, Markdown, and more Separation of Concerns: Clean split between computation and delivery Repeatability: Deterministic, parameterized file generation Governance-Friendly: Outputs are traceable, reproducible, and auditable The EXPORTS block completes the ETLX pipeline by turning validated, curated data into consumable artifacts.\n"
            }
        );
    index.add(
            {
                id:  10 ,
                href: "\/etlxdocs\/docs\/features\/actions\/",
                title: "Actions",
                description: "Define file operations and external transfers in ETLX workflows",
                content: "Actions linkThere are scenarios in ETL workflows where actions such as downloading, uploading, compressing or copying files cannot be performed using SQL alone. The ACTIONS section allows you to define steps for copying or transferring files using the file system or external protocols.\nACTIONS Structure linkEach action under the ACTIONS section has the following:\nname: Unique name for the action. description: Human-readable explanation. type: The kind of action to perform. Options: copy_file compress decompress ftp_download ftp_upload sftp_download sftp_upload http_download http_upload s3_download s3_upload db_2_db params: A map of input parameters required by the action type. # ACTIONS\r```yaml metadata\rname: FileOperations\rdescription: \"Transfer and organize generated reports\"\rpath: examples\ractive: true\r```\r## COPY LOCAL FILE\r```yaml metadata\rname: CopyReportToArchive\rdescription: \"Move final report to archive folder\"\rtype: copy_file\rparams:\rsource: \"/reports/final_report.xlsx\"\rtarget: \"/reports/archive/final_report_YYYYMMDD.xlsx\"\ractive: true\r```\r## Compress to ZIP\r```yaml metadata\rname: CompressReports\rdescription: \"Compress report files into a .zip archive\"\rtype: compress\rparams:\rcompression: zip\rfiles:\r- \"reports/report_1.csv\"\r- \"reports/report_2.csv\"\routput: \"archives/reports_YYYYMM.zip\"\ractive: true\r```\r## UNZIP\r```yaml metadata\rname: CompressReports\rdescription: \"Compress report files into a .zip archive\"\rtype: decompress\rparams:\rcompression: zip\rinput: \"archives/reports_YYYYMM.zip\"\routput: \"tmp\"\ractive: true\r```\r## Compress to GZ\r```yaml metadata\rname: CompressToGZ\rdescription: \"Compress a summary file to .gz\"\rtype: compress\rparams:\rcompression: gz\rfiles:\r- \"reports/summary.csv\"\routput: \"archives/summary_YYYYMM.csv.gz\"\ractive: true\r```\r## HTTP DOWNLOAD\r```yaml metadata\rname: DownloadFromAPI\rdescription: \"Download dataset from HTTP endpoint\"\rtype: http_download\rparams:\rurl: \"https://api.example.com/data\"\rtarget: \"data/today.json\"\rmethod: GET\rheaders:\rAuthorization: \"Bearer @API_TOKEN\"\rAccept: \"application/json\"\rparams:\rdate: \"YYYYMMDD\"\rlimit: \"1000\"\ractive: true\r```\r## HTTP UPLOAD\r```yaml metadata\rname: PushReportToWebhook\rdescription: \"Upload final report to an HTTP endpoint\"\rtype: http_upload\rparams:\rurl: \"https://webhook.example.com/upload\"\rmethod: POST\rsource: \"reports/final.csv\"\rheaders:\rAuthorization: \"Bearer @WEBHOOK_TOKEN\"\rContent-Type: \"multipart/form-data\"\rparams:\rtype: \"summary\"\rdate: \"YYYYMMDD\"\ractive: true\r```\r## FTP DOWNLOAD\r```yaml metadata\rname: FetchRemoteReport\rdescription: \"Download data file from external FTP\"\rtype: ftp_download\rparams:\rhost: \"ftp.example.com\"\rport: \"21\"\ruser: \"myuser\"\rpassword: \"@FTP_PASSWORD\"\rsource: \"/data/daily_report.csv\"\rtarget: \"downloads/daily_report.csv\"\ractive: true\r```\r## FTP DOWNLOAD GLOB\r```yaml metadata\rname: FetchRemoteReport2024\rdescription: \"Download data file from external FTP\"\rtype: ftp_download\rparams:\rhost: \"ftp.example.com\"\rport: \"21\"\ruser: \"myuser\"\rpassword: \"@FTP_PASSWORD\"\rsource: \"/data/daily_report_2024*.csv\"\rtarget: \"downloads/\"\ractive: true\r```\r## SFTP DOWNLOAD\r```yaml metadata\rname: FetchRemoteReport\rdescription: \"Download data file from external SFTP\"\rtype: stp_download\rparams:\rhost: \"sftp.example.com\"\ruser: \"myuser\"\rpassword: \"@SFTP_PASSWORD\"\rhost_key: ~/.ssh/known_hosts # or a specific file\rport: 22\rsource: \"/data/daily_report.csv\"\rtarget: \"downloads/daily_report.csv\"\ractive: true\r```\r## S3 UPLOAD\r```yaml metadata\rname: ArchiveToS3\rdescription: \"Send latest results to S3 bucket\"\rtype: s3_upload\rparams:\rAWS_ACCESS_KEY_ID: '@AWS_ACCESS_KEY_ID'\rAWS_SECRET_ACCESS_KEY: '@AWS_SECRET_ACCESS_KEY'\rAWS_REGION: '@AWS_REGION'\rAWS_ENDPOINT: 127.0.0.1:3000\rS3_FORCE_PATH_STYLE: true\rS3_DISABLE_SSL: false\rS3_SKIP_SSL_VERIFY: true\rbucket: \"my-etlx-bucket\"\rkey: \"exports/summary_YYYYMMDD.xlsx\"\rsource: \"reports/summary.xlsx\"\ractive: true\r```\r## S3 DOWNLOAD\r```yaml metadata\rname: DownalodFromS3\rdescription: \"Download file S3 from bucket\"\rtype: s3_download\rparams:\rAWS_ACCESS_KEY_ID: '@AWS_ACCESS_KEY_ID'\rAWS_SECRET_ACCESS_KEY: '@AWS_SECRET_ACCESS_KEY'\rAWS_REGION: '@AWS_REGION'\rAWS_ENDPOINT: 127.0.0.1:3000\rS3_FORCE_PATH_STYLE: true\rS3_DISABLE_SSL: false\rS3_SKIP_SSL_VERIFY: true\rbucket: \"my-etlx-bucket\"\rkey: \"exports/summary_YYYYMMDD.xlsx\"\rtarget: \"reports/summary.xlsx\"\ractive: true\r``` ðŸ“¥ ACTIONS â€“ db_2_db (Cross-Database Write) link As of this moment, DuckDB does not support direct integration with certain databases like MSSQL, DB2, or Oracle, the same way it does with SQLite, Postgres, or MySQL.\nTo bridge this gap, the db_2_db action type allows you to query data from one database (source) and write the results into another (target), using ETLXâ€™s internal execution engine (powered by sqlx or ODBC).\nâœ… Use Case linkUse db_2_db when:\nYour database is not accessible with DuckDB. You want to move data from one place to another using pure SQL, chunked if necessary. ðŸ§© Example link ...\r## WRITE_RESULTS_MSSQL\r```yaml metadata\rname: WRITE_RESULTS_MSSQL\rdescription: \"MSSQL example â€“ moving logs into a SQL Server database.\"\rtype: db_2_db\rparams:\rsource:\rconn: sqlite3:database/HTTP_EXTRACT.db\rbefore: null\rchunk_size: 1000\rtimeout: 30\rsql: origin_query\rafter: null\rtarget:\rconn: mssql:sqlserver://sa:@MSSQL_PASSWORD@localhost?database=master\u0026connection+timeout=30\rtimeout: 30\rbefore:\r- create_schema\rsql: mssql_sql\rafter: null\ractive: true\r```\r```sql\r-- origin_query\rSELECT \"description\", \"duration\", STRFTIME('%Y-%m-%d %H:%M:%S', \"start_at\") AS \"start_at\", \"ref\"\rFROM \"etlx_logs\" ORDER BY \"start_at\" DESC\r```\r```sql\r-- create_schema\rIF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'etlx_logs' AND type = 'U')\rCREATE TABLE [dbo].[etlx_logs] (\r[description] NVARCHAR(MAX) NULL,\r[duration] BIGINT NULL,\r[start_at] DATETIME NULL,\r[ref] DATE NULL\r);\r```\r```sql\r-- mssql_sql\rINSERT INTO [dbo].[etlx_logs] ([:columns]) VALUES ``` ðŸ› ï¸ Notes link You can define before and after SQL on both source and target sides. [:columns] will be automatically expanded with the column list. Data is inserted in chunks using the provided chunk_size. Compatible with any driver supported by sqlx or databse tahat has an ODBC driver. ðŸ“ Note: All paths and dynamic references (like YYYYMMDD) are replaced at runtime by the refered date.\nYou can use environmental variables via @ENV_NAME.\nâš ï¸ Note on S3 Configuration linkWhen using s3_upload or s3_download, ETLX will look for the required AWS credentials and config in the parameters you provide in your ACTIONS block, such as:\nAWS_ACCESS_KEY_ID: '@AWS_ACCESS_KEY_ID'\rAWS_SECRET_ACCESS_KEY: '@AWS_SECRET_ACCESS_KEY'\rAWS_REGION: '@AWS_REGION'\rAWS_ENDPOINT: '127.0.0.1:3000'\rS3_FORCE_PATH_STYLE: true\rS3_DISABLE_SSL: false\rS3_SKIP_SSL_VERIFY: true ðŸ§  If these parameters are not explicitly defined, ETLX will fall back to the systemâ€™s environment variables with the same names. This allows for better compatibility with tools like AWS CLI, Docker secrets, and .env files.\nThis behavior ensures flexible support for local development, staging environments, and production deployments where credentials are injected at runtime.\nâš ï¸ Security Warning: User-Defined Actions link â— Dangerous if misused\nAllowing users to define or influence ACTIONS (e.g. file copy, upload, or download steps) introduces potential security risks such as:\nArbitrary file access or overwrite Sensitive file exposure (e.g. /etc/passwd) Remote execution or data exfiltration ðŸ” Best Practices link Restrict file paths using whitelists (AllowedPaths) or path validation. Never accept unvalidated user input for action parameters like source, target, or url. Use readonly or sandboxed environments when possible. Log and audit every ACTIONS block executed in production. ðŸ“Œ If youâ€™re using ETLX as a library (Go or Python), you must sanitize and scope what the runtime has access to.\n"
            }
        );
    index.add(
            {
                id:  11 ,
                href: "\/etlxdocs\/docs\/features\/scripts\/",
                title: "Scripts",
                description: "Execute SQL statements for operational and maintenance tasks",
                content: "Scripts linkThe SCRIPTS section allows you to execute SQL statements that do not naturally belong to other ETLX blocks such as ETL, DATA_QUALITY, EXPORTS, or MULTI_QUERIES. It is designed for operational, maintenance, and orchestration-style SQL, where the goal is execution rather than producing datasets.\nTypical use cases include cleanup operations, database maintenance, schema adjustments, or any SQL that should run as part of a pipeline but does not produce query results.\nWhen to Use SCRIPTS linkUse the SCRIPTS block when you need to:\nRun cleanup queries after ETL or transformation steps Execute ad-hoc or scheduled maintenance tasks Perform DDL or administrative SQL (DROP, VACUUM, ANALYZE, etc.) Run SQL that does not need to return rows Apply post-processing logic not tied to a dataset If your logic produces data that should be queried, validated, or exported, prefer other ETLX blocks. SCRIPTS is intentionally simple and execution-focused.\nScripts Structure linkThe SCRIPTS block follows the same hierarchical pattern used across ETLX:\nTop-level SCRIPTS block\nDefines shared metadata such as connection and activation status. Script definitions (Level 2 headings)\nEach script represents a single executable SQL unit. Scripts can override connection details and lifecycle hooks. Execution lifecycle\nbefore_sql commands are executed first The script SQL is executed after_sql commands are executed last Scripts are executed sequentially and do not return results.\nScripts Markdown Example linkThe following example demonstrates how to run cleanup scripts after an ETL process.\n# MANTAINANCE\rRun Queries that does not need a return\r```yaml metadata\rname: DailyScripts\rruns_as: SCRIPTS\rdescription: \"Daily Scripts\"\rconnection: \"duckdb:\"\ractive: true\r```\r## SCRIPT1\r```yaml metadata\rname: SCRIPT1\rdescription: \"Clean up auxiliar / temp data\"\rconnection: \"duckdb:\"\rbefore_sql: \"ATTACH 'database/DB.db' AS DB (TYPE SQLITE)\"\rscript_sql: clean_aux_data\ron_err_patt: null\ron_err_sql: null\rafter_sql: \"DETACH DB\"\ractive: true\r```\r```sql\r-- clean_aux_data\rDROP TEMP_TABLE1;\r``` How Scripts Work link Initialization\nLoads required extensions Establishes database connections Execution\nExecutes the SQL referenced by script_sql No result set is expected or captured Lifecycle Hooks\nbefore_sql runs before execution after_sql runs after execution Error Handling\nOptional on_err_patt and on_err_sql allow conditional recovery logic Example Use Cases link Dropping temporary or staging tables Vacuuming or analyzing databases Refreshing materialized views Applying schema migrations Executing post-load cleanup logic Scripts Benefits link Operational Flexibility Execute any SQL without forcing it into a data-oriented block.\nClear Separation of Concerns Keeps maintenance and orchestration logic isolated from ETL and analytics.\nPipeline Integration Scripts run as first-class citizens in ETLX workflows.\nConsistent Lifecycle Model Uses the same metadata and execution hooks as other ETLX blocks.\nThe SCRIPTS block is intentionally minimal, powerful, and predictableâ€”making it ideal for operational SQL that supports, but does not define, your data products.\n"
            }
        );
    index.add(
            {
                id:  12 ,
                href: "\/etlxdocs\/docs\/features\/logs\/",
                title: "Logs / Observability",
                description: "ETLX logging mechanism to save logs into a database.",
                content: "Logs / Observability Handling (# LOGS) linkETLX provides a logging mechanism that allows saving logs into a database. This is useful for tracking executions, debugging, and auditing ETL processes.\nBy default, ETLX logs every relevant aspect of the pipeline:\nStep name / description Timestamp start / end Duration in seconds Success / failure Messages and errors Memory usage Optional row counts, files generated, or other metrics All logs are serialized as JSON and persisted using the LOGS block.\nðŸ”¹ How It Works link The LOGS section defines where and how logs should be saved. The logging lifecycle consists of three main steps: Prepare the environment using before_sql (load extensions, attach databases) Execute save_log_sql to store logs in the database Run after_sql for cleanup (detach DB, free temp resources) Logs are emitted regardless of pipeline success to guarantee observability. ðŸ›  Example LOGS Configuration linkBelow is an example that saves logs into a database:\n# LOGS\r```yaml metadata\rname: LOGS\rdescription: \"Example saving logs\"\rtable: logs\rconnection: \"duckdb:\"\rbefore_sql: \"ATTACH 'examples/S3_EXTRACT.db' AS \"DB\" (TYPE SQLITE)\"\rsave_log_sql: load_logs\rsave_on_err_patt: '(?i)table.+with.+name.+(\\w+).+does.+not.+exist'\rsave_on_err_sql: create_logs\rafter_sql: 'DETACH \"DB\"'\ractive: true\r```\r```sql\r-- load_logs\rINSERT INTO \"DB\".\"\" BY NAME\rSELECT * FROM READ_JSON('');\r```\r```sql\r-- create_logs\rCREATE OR REPLACE TABLE \"DB\".\"\" AS\rSELECT * FROM READ_JSON('');\r``` ðŸ”¹ How to Use link This example saves logs into a SQLite database attached to DuckDB. The log table (logs) is created or replaced on each run. The and placeholders are dynamically replaced. Default Logs (AUTO_LOGS) linkIf you donâ€™t define a LOGS block, ETLX injects AUTO_LOGS automatically:\n# AUTO_LOGS\r```yaml metadata\rname: LOGS\rdescription: \"Logging\"\rtable: logs\rconnection: \"duckdb:\"\rbefore_sql:\r- \"LOAD Sqlite\"\r- \"ATTACH '/etlx_logs.db' (TYPE SQLITE)\"\r- \"USE etlx_logs\"\r- \"LOAD json\"\r- \"get_dyn_queries[create_missing_columns](ATTACH '/etlx_logs.db' (TYPE SQLITE),DETACH etlx_logs)\"\rsave_log_sql: |\rINSERT INTO \"etlx_logs\".\"\" BY NAME\rSELECT *\rFROM READ_JSON('');\rsave_on_err_patt: '(?i)table.+with.+name.+(\\w+).+does.+not.+exist'\rsave_on_err_sql: |\rCREATE TABLE \"etlx_logs\".\"\" AS\rSELECT *\rFROM READ_JSON('');\rafter_sql:\r- 'USE memory'\r- 'DETACH \"etlx_logs\"'\ractive: true\r```\r```sql\r-- create_missing_columns\rWITH source_columns AS (\rSELECT \"column_name\", \"column_type\"\rFROM (DESCRIBE SELECT * FROM READ_JSON(''))\r),\rdestination_columns AS (\rSELECT \"column_name\", \"data_type\" as \"column_type\"\rFROM \"duckdb_columns\"\rWHERE \"table_name\" = ''\r),\rmissing_columns AS (\rSELECT \"s\".\"column_name\", \"s\".\"column_type\"\rFROM source_columns \"s\"\rLEFT JOIN destination_columns \"d\" ON \"s\".\"column_name\" = \"d\".\"column_name\"\rWHERE \"d\".\"column_name\" IS NULL\r)\rSELECT 'ALTER TABLE \"etlx_logs\".\"\" ADD COLUMN \"' || \"column_name\" || '\" ' || \"column_type\" || ';' AS \"query\"\rFROM missing_columns\rWHERE (SELECT COUNT(*) FROM destination_columns) \u003e 0;\r``` âœ… Key Points link Keeps a persistent log of all ETLX executions Uses DuckDB for efficient storage Supports preprocessing (before_sql) and cleanup (after_sql) Supports automatic schema evolution Default logging is enabled, but can be overridden Logs every metric, message, and error for observability ðŸ”§ Customizing Logs linkTo save logs to your own database:\nATTACH 'my_custom_logs.db' (TYPE SQLITE) Or point to any DuckDB-supported database, e.g., Postgres or MySQL, by updating the connection string.\nðŸ“Š Observability Use Cases link Full execution timelines ETL duration per step File, network, or DB operation metrics Auditing user-defined actions Feeding BI dashboards or monitoring systems "
            }
        );
    index.add(
            {
                id:  13 ,
                href: "\/etlxdocs\/docs\/features\/notify\/",
                title: "Notifications",
                description: "Send notifications with dynamic templates from SQL query results",
                content: "Notifications linkThe NOTIFY section enables sending notifications (e.g., email via SMTP) with dynamic templates populated from SQL query results. This is useful for monitoring ETL processes and sending status reports.\nWhy Use NOTIFY? linkâœ… Real-time updates on ETL status\nâœ… Customizable email templates with dynamic content\nâœ… Supports attachments for automated reporting\nâœ… Ensures visibility into ETL success or failure\nExample: Sending ETL Status via Email linkThis example sends an email after an ETL process completes, using log data from the database.\nNOTIFY Markdown Configuration link # NOTIFY\r```yaml metadata\rname: Notification\rdescription: \"ETL Notification\"\rconnection: \"duckdb:\"\rpath: \"examples\"\ractive: true\r```\r## ETL_STATUS\r```yaml metadata\rname: ETL_STATUS\rdescription: \"ETL Status\"\rconnection: \"duckdb:\"\rbefore_sql:\r- \"INSTALL sqlite\"\r- \"LOAD sqlite\"\r- \"ATTACH 'database/HTTP_EXTRACT.db' AS DB (TYPE SQLITE)\"\rdata_sql:\r- logs\rafter_sql: \"DETACH DB\"\rto:\r- real.datadriven@gmail.com\rcc: null\rbcc: null\rsubject: 'ETLX YYYYMMDD'\rbody: body_tml\rattachments:\r- hf.md\r- http.md\ractive: true\r```\rThe **email body** is defined using a **Golang template**. The results from `data_sql` are available inside the template, also the Spring (`github.com/Masterminds/sprig`) library that provides more than 100 commonly used template functions.\r```html body_tml\rGood Morning!\nThis email was gebnerated by ETLX automatically!\nLOGS:\n{{ with .logs }}\r{{ if eq .success true }}\rName\rRef\rStart\rEnd\rDuration\rSuccess\rMessage\r{{ range .data }}\r{{ .name }}\r{{ .ref }}\r{{ .start_at | date \"2006-01-02 15:04:05\" }}\r{{ .end_at | date \"2006-01-02 15:04:05\" }}\r{{ divf .duration 1000000000 | printf \"%.4fs\" }}\r{{ .success }}\r{{ .msg | toString | abbrev 30}}\r{{ else }}\rNo items available\r{{ end }}\r{{ else }}\r{{.msg}}\n{{ end }}\r{{ else }}\rLogs information missing.\n{{ end }}\r```\r```sql\r-- logs\rSELECT *\rFROM \"DB\".\"etlx_logs\"\rWHERE \"ref\" = '{YYYY-MM-DD}'\r``` How NOTIFY Works link1ï¸âƒ£ Loads required extensions and connects to the database (before_sql).\n2ï¸âƒ£ Executes data_sql queries to retrieve data to be embeded in the body of the email.\n3ï¸âƒ£ Uses the results inside the body template (Golang templating).\n4ï¸âƒ£ Sends an email with the formatted content and attachments.\n5ï¸âƒ£ Executes cleanup queries (after_sql).\nKey NOTIFY Features linkâœ” Dynamic email content populated from SQL queries\nâœ” Supports to, cc, bcc, attachments, and templated bodies\nâœ” Executes SQL before and after sending notifications\nâœ” Ensures ETL monitoring and alerting\n"
            }
        );
    index.add(
            {
                id:  14 ,
                href: "\/etlxdocs\/docs\/features\/require\/",
                title: "Requires",
                description: "Load configuration dependencies from files or queries.",
                content: "Requires linkThe REQUIRES section in the ETL configuration allows you to load dependencies from external Markdown configurations. These dependencies can either be loaded from file paths or dynamically through queries. This feature promotes modularity and reusability by enabling you to define reusable parts of the configuration in separate files or queries.\nLoading Structure link Metadata:\nThe REQUIRES section includes metadata describing its purpose and activation status. Loading Options:\nFrom Queries: Dynamically fetch configuration content from a query. Specify the query, column containing the configuration, and optional pre/post SQL scripts. From Files: Load configuration content from an external file path. Integration:\nThe loaded configuration is merged into the main configuration. Top-level headings in the required configuration that donâ€™t exist in the main configuration are added. Loading Markdown Example link # REQUIRES\r```yaml\rdescription: \"Load configuration dependencies from files or queries.\"\ractive: true\r```\r## Sales Transformation\r```yaml\rname: SalesTransform\rdescription: \"Load sales transformation config from a query.\"\rconnection: \"duckdb:\"\rbefore_sql:\r- \"LOAD sqlite\"\r- \"ATTACH 'reporting.db' AS DB (TYPE SQLITE)\"\rquery: get_sales_conf\rcolumn: md_conf_content # Defaults to 'conf' if not provided.\rafter_sql: \"DETACH DB\"\ractive: false\r```\r```sql\r-- get_sales_conf\rSELECT \"md_conf_content\"\rFROM \"configurations\"\rWHERE \"config_name\" = 'Sales'\rAND \"active\" = true\rAND \"excluded\" = false;\r```\r## Inventory Transformation\r```yaml\rname: InventoryTransform\rdescription: \"Load inventory transformation config from a file.\"\rpath: \"/path/to/Configurations/inventory_transform.md\"\r_path: \"/path/to/sql/query/inventory_transform.sql\" # Optional associated SQL file path.\ractive: true\r``` How Loading Works link Defining Dependencies:\nDependencies are listed as child sections under the # REQUIRES heading. Each dependency specifies its source (query or path) and associated metadata. From Queries:\nUse the query field to specify a SQL query that retrieves the configuration. The column field specifies which column contains the Markdown configuration content. Optionally, use before_sql and after_sql to define scripts to run before or after executing the query. From Files:\nUse the path field to specify the file path of an external Markdown configuration. Merging with Main Configuration:\nAfter loading the configuration, any top-level headings in the loaded configuration that donâ€™t exist in the main configuration are added. Loading - Example Use Case linkFor the example above, the following happens:\nSales Transformation:\nA query retrieves the Markdown configuration content for sales transformations from a database table. The before_sql and after_sql scripts prepare the environment for the query execution. Inventory Transformation:\nA Markdown configuration is loaded from an external file path (/path/to/Configurations/inventory_transform.md). Loading - Benefits link Modularity: Break large configurations into smaller, reusable parts. Dynamic Updates: Use queries to dynamically load updated configurations from databases. Ease of Maintenance: Keep configurations for different processes in separate files or sources, simplifying updates and version control. By leveraging the REQUIRES section, you can maintain a clean and scalable ETL configuration structure, promoting reusability and modular design.\n"
            }
        );
    index.add(
            {
                id:  15 ,
                href: "\/etlxdocs\/docs\/features\/advanced\/",
                title: "Advanced Features",
                description: "Explore advanced ETLX features like dynamic query generation, conditional execution, and modular configurations.",
                content: "Advanced Features link Error Handling: Define patterns for resolving errors dynamically during execution.\nload_on_err_match_patt: \"(?i)table.+does.+not.+exist\"\rload_on_err_match_sql: \"CREATE TABLE sales_table (id INT, total FLOAT)\" Modular Configuration: Break down workflows into reusable components for better maintainability.\nðŸ› ï¸ Advanced Usage: Dynamic Query Generation (get_dyn_queries[...]) linkIn some advanced ETL workflows, you may need to dynamically generate SQL queries based on metadata or schema differences between the source and destination databases.\nðŸ”¹ Why Use Dynamic Queries? linkâœ… Schema Flexibility â€“ Automatically adapt to schema changes in the source system.\nâœ… Self-Evolving Workflows â€“ ETL jobs can generate and execute additional SQL queries as needed.\nâœ… Automation â€“ Reduces the need for manual intervention when new columns appear.\nðŸ”¹ How get_dyn_queries[query_name](runs_before,runs_after) Works link Dynamic queries are executed using the get_dyn_queries[query_name](runs_before,runs_after) pattern. During execution, ETLX runs the query query_name and retrieves dynamically generated queries. The resulting queries are then executed automatically. ðŸ›  Example: Auto-Adding Missing Columns linkThis example checks for new columns in a JSON file and adds them to the destination table.\nðŸ“„ Markdown Configuration for get_dyn_queries[query_name](runs_before,runs_after) link If the query_name depends on attaching and detaching the main db where it will run, those should be passed as dependencies, because the dynamic queries are generate before any other query and put in the list for the list where it is to be executed, to be a simpler flow, but they are optional otherwise.\n....\r```yaml metadata\r...\rconnection: \"duckdb:\"\rbefore_sql:\r- ...\r- get_dyn_queries[create_missing_columns] # Generates queries defined in `create_missing_columns` and Executes them\r..\r```\r**ðŸ“œ SQL Query (Generating Missing Columns)**\r```sql\r-- create_missing_columns\rWITH source_columns AS (\rSELECT column_name, column_type FROM (DESCRIBE SELECT * FROM read_json(''))\r),\rdestination_columns AS (\rSELECT column_name, data_type as column_type\rFROM duckdb_columns WHERE table_name = ''\r),\rmissing_columns AS (\rSELECT s.column_name, s.column_type\rFROM source_columns s\rLEFT JOIN destination_columns d ON s.column_name = d.column_name\rWHERE d.column_name IS NULL\r)\rSELECT 'ALTER TABLE \"\" ADD COLUMN \"' || column_name || '\" ' || column_type || ';' AS query\rFROM missing_columns\rWHERE (SELECT COUNT(*) FROM destination_columns) \u003e 0;\r```\r... ðŸ›  Execution Flow link1ï¸âƒ£ Extract column metadata from the input (in this case a json file, but it could be a table or any other valid query).\n2ï¸âƒ£ Check which columns are missing in the destination table ().\n3ï¸âƒ£ Generate ALTER TABLE statements for adding missing columns, and replaces the - get_dyn_queries[create_missing_columns] with the the generated queries\n4ï¸âƒ£ Runs the workflow with dynamically generated queries against the destination connection.\nðŸ”¹ Key Features linkâœ” Fully automated schema updates\nâœ” Works with flexible schema data (e.g., JSON, CSV, Parquet, etc.)\nâœ” Reduces manual maintenance when source schemas evolve\nâœ” Ensures destination tables always match source structure\nWith get_dyn_queries[...], your ETLX workflows can now dynamically evolve with changing data structures!\nðŸ”„ Conditional Execution linkETLX allows conditional execution of SQL blocks based on the results of a query. This is useful to skip operations dynamically depending on data context (e.g., skip a step if no new data is available, or if a condition in the target is not met.\nYou can define condition blocks using the following keys:\nFor ETL step-specific conditions:\nextract_condition transform_condition load_condition etc. For generic sections (e.g., DATA_QUALITY, EXPORTS, NOTIFY, etc.):\ncondition You can also specify an optional *condition_msg to log a custom message when a condition is not met.\nCondition Evaluation Logic link The SQL query defined in *_condition or condition is executed. The result must mast be boolean. If not met, the corresponding main logig will be skipped. If *_condition_msg is provided, it will be included in the log entry instead of the default skip message. Example â€“ Conditional Load Step link load_conn: \"duckdb:\"\rload_condition: check_load_required\rload_condition_msg: \"No new records to load today\"\rload_sql: perform_load -- check_load_required\rSELECT COUNT(*) \u003e 0 as _check FROM staging_table WHERE processed = false; -- perform_load\rINSERT INTO target_table\rSELECT * FROM staging_table WHERE processed = false; Example - Global Conditional Notification link description: \"Send email only if failures occurred\"\rconnection: \"duckdb:\"\rcondition: check_failures\rcondition_msg: \"No failures detected, no email sent\" -- check_failures\rSELECT COUNT(*) \u003e 0 as chk FROM logs WHERE success = false; ðŸ“ Note: If no *_condition_msg is defined and the condition fails, ETLX will simply log the skipped step with a standard message like:\n\"Condition 'load_condition' was not met. Skipping step 'load'.\"\nAdvanced Workflow Execution: runs_as Override linkBy default, the ETLX engine processes each Level 1 section (like ETL, DATA_QUALITY, EXPORTS, ACTIONS, LOGS, NOTIFY etc.) in the order that order. However, in more advanced workflows, it is often necessary to:\nExecute a second ETL process after quality validations (DATA_QUALITY). Reuse intermediate outputs within the same config, without having to create and chain multiple .md config files. To enable this behavior, ETLX introduces the runs_as field in the metadata block of any Level 1 key. This tells ETLX to treat that section as if it were a specific built-in block like ETL, EXPORTS, etc., even if it has a different name.\n# ETL_AFTER_SOME_KEY\r```yaml metadata\rruns_as: ETL\rdescription: \"Post-validation data transformation\"\ractive: true\r```\r## ETL_OVER_SOME_MAIN_STEP\r... In this example:\nETLX will run the original ETL block. Then execute DATA_QUALITY, an so on. Then treat ETL_AFTER_SOME_KEY as another ETL block (since it contains runs_as: ETL) and execute it as such. This allows chaining of processes within the same configuration file.\nâš ï¸ Order Matters linkThe custom section (e.g. # ETL_AFTER_SOME_KEY) is executed in the order it appears in the Markdown file after the main keys. That means the flow becomes:\n# ETL # DATA_QUALITY # ETL2 (runs as ETL) This enables advanced chaining like:\nExporting logs after validation. Reapplying transformations based on data quality feedback. Generating post-validation reports. "
            }
        );
    index.add(
            {
                id:  16 ,
                href: "\/etlxdocs\/docs\/features\/embedding\/",
                title: "Embedding in GO",
                description: "Embedding ETLX in Go applications for seamless integration.",
                content: "Embedding in Go linkTo embed the ETL framework in a Go application, you can use the etlx package and call ConfigFromMDText and RunETL. Example (from README):\npackage main\rimport (\r\"fmt\"\r\"time\"\r\"github.com/realdatadriven/etlx\"\r)\rfunc main() {\retl := \u0026etlx.ETLX{}\r// Load configuration from Markdown text\rerr := etl.ConfigFromMDText(`# Your Markdown config here`)\rif err != nil {\rfmt.Printf(\"Error loading config: %v\\n\", err)\rreturn\r}\r// Prepare date reference\rdateRef := []time.Time{time.Now().AddDate(0, 0, -1)}\r// Define additional options\roptions := map[string]any{\r\"only\": []string{\"sales\"},\r\"steps\": []string{\"extract\", \"load\"},\r}\r// Run ETL process\rlogs, err := etl.RunETL(dateRef, nil, options)\rif err != nil {\rfmt.Printf(\"Error running ETL: %v\\n\", err)\rreturn\r}\r// Print logs\rfor _, log := range logs {\rfmt.Printf(\"Log: %+v\\n\", log)\r}\r} This code snippet demonstrates how to set up and run an ETL process using the ETLX framework within a Go application. You can customize the configuration and options as needed for your specific use case.\nThe binary etlx (see realdatadriven/etlx/cmd/main.go) itself also uses this embedding approach internally to run ETL processes defined in Markdown files .\n"
            }
        );
    index.add(
            {
                id:  17 ,
                href: "\/etlxdocs\/docs\/features\/ddb-at-the-core\/",
                title: "DuckDB at the Core",
                description: "SQL-first transformations and in-process analytics powered by DuckDB",
                content: "DuckDB at the Core linkETLX is built around DuckDB as its execution engine. At its core, ETLX embraces a SQL-first philosophy, enabling powerful in-process analytics and transformations without requiring external compute engines or distributed systems.\nDuckDB acts as the analytical backbone of the pipeline, executing transformations, validations, exports, and even orchestration-related logic using standard SQL.\nðŸ§  Why DuckDB? linkDuckDB is a modern analytical database designed for OLAP workloads, embedded directly into applications. This makes it a perfect fit for ETLX.\nKey advantages:\nEmbedded \u0026 In-process â€“ No external service to manage Columnar execution â€“ High performance for analytical queries Rich SQL support â€“ Window functions, CTEs, complex joins Extensible â€“ Load extensions (SQLite, Postgres, Excel, JSON, HTTP, Parquet, etc.) Portable â€“ Same SQL runs locally, in CI, or in production ETLX leverages all of these features while keeping the workflow declarative and reproducible.\nðŸ”¹ SQL-First Transformations linkIn ETLX, SQL is the primary transformation language.\nThis means:\nBusiness logic is written in plain SQL Transformations are self-documented Pipelines are easier to review, audit, and version-control Example:\nSELECT\rcustomer_id,\rSUM(amount) AS total_spent,\rCOUNT(*) AS total_orders\rFROM sales\rGROUP BY customer_id; No DSLs. No custom operators. Just SQL.\nðŸ”¹ In-Process Analytics linkBecause DuckDB runs inside the ETLX process, analytics are:\nExecuted in-memory or on local disk Free from network latency Deterministic and easy to debug This enables advanced use cases such as:\nAd-hoc analytics during ETL Data quality checks Profiling and statistics generation Report aggregation Metadata-driven validation All without shipping data to an external engine.\nðŸ”¹ One Engine, Multiple Data Sources linkDuckDB allows ETLX to query multiple data sources using SQL:\nLocal files (CSV, Parquet, JSON, Excel) SQLite databases Postgres / MySQL (via extensions) HTTP / S3-compatible storage Example:\nSELECT *\rFROM read_parquet('s3://bucket/data/*.parquet'); This enables federated analytics while keeping a single execution engine.\nðŸ”¹ Foundation for ETLX Features linkDuckDB powers almost every ETLX feature:\nETL / ELT transformations DATA_QUALITY validations MULTI_QUERIES aggregation EXPORTS (CSV, Excel, templates) LOGS persistence SCRIPTS execution By standardizing on DuckDB, ETLX ensures consistent behavior across all pipeline stages.\nðŸŽ¯ Design Philosophy link If it can be expressed in SQL, ETLX should execute it.\nDuckDB enables ETLX to remain:\nMinimal â€“ fewer moving parts Transparent â€“ SQL is visible and inspectable Performant â€“ optimized analytical execution Portable â€“ runs anywhere âœ… Summary link DuckDB is the core execution engine of ETLX Enables SQL-first, declarative pipelines Provides in-process analytics with no external dependencies Powers transformations, validations, exports, and observability DuckDB is not just a dependency in ETLX â€” it is the foundation.\n"
            }
        );
    index.add(
            {
                id:  18 ,
                href: "\/etlxdocs\/docs\/features\/multi-engine-execution\/",
                title: "Multi-Engine Execution",
                description: "Run ETLX pipelines across DuckDB, PostgreSQL, SQLite, MySQL, SQL Server, and any database supported by sqlx or ODBC.",
                content: " Multi-Engine Execution linkETLX is designed to be engine-agnostic by default. While DuckDB is the recommended and primary execution engine, ETLX can execute pipelines across multiple database engines within the same workflow, depending on availability, constraints, and use cases.\nThis allows ETLX to operate:\nFully embedded and in-process (DuckDB, SQLite) Against external OLTP / analytical databases (PostgreSQL, MySQL, SQL Server) Through ODBC or other sqlx-supported drivers for broader compatibility ðŸ§  DuckDB is a developer choice, not a hard dependency. ETLX adapts to your environment instead of forcing a single execution engine.\nSupported Execution Engines linkETLX supports the following execution backends:\nEngine Mode Notes DuckDB Embedded Default engine, SQL-first analytics, file-based I/O, best performance SQLite Embedded Lightweight storage, logs, metadata, small datasets PostgreSQL External OLTP / analytical workloads MySQL / MariaDB External Operational databases SQL Server (MSSQL) External Enterprise systems via sqlx or ODBC Any sqlx-supported DB External Any database that has support https://github.com/jmoiron/sqlx ODBC sources External Legacy systems, Excel, proprietary engines Execution Model linkEvery executable step in ETLX explicitly declares which engine to use.\nThis is done using connection fields such as:\nconnection _conn source.conn / target.conn (in db_2_db actions) If a connection is not specified, ETLX falls back to the pipeline default engine.\nconnection: \"duckdb:\" # default DuckDB as the Default Engine linkDuckDB is embedded directly into the ETLX process and provides:\nIn-process execution (no external service) Excellent performance for analytical queries Native support for files (CSV, Parquet, JSON, Excel) Cross-database access via extentions Typical ETLX workflows use DuckDB to:\nExtract from multiple sources Transform data using SQL Export files Run validations Persist logs connection: \"duckdb:\" Running Pipelines Without DuckDB linkAlthough DuckDB is recommended, ETLX does not require it.\nYou can execute most pipeline steps directly on:\nPostgreSQL MySQL SQL Server SQLite Example using PostgreSQL as the primary engine:\nconnection: \"postgres:dbname=erpdb host=db user=etl password=@PG_PASS\" This allows ETLX to act as a pure SQL execution and orchestration layer on top of an existing database.\nâš ï¸ Some features (file exports, multi-engine joins, advanced analytics) may be limited without DuckDB.\nMixing Engines in a Single Pipeline linkETLX supports multi-engine pipelines.\nExample:\nExtract from PostgreSQL Transform in DuckDB Export to files Write results back to SQL Server extract_conn: \"postgres:...\"\rload_conn: \"duckdb:\"\rtransform_conn: \"duckdb:\" This pattern is common when:\nSource systems are operational databases Transformations are analytical Outputs are files or reports Cross-Database Transfers (ACTION:db_2_db) linkWhen a database cannot be accessed directly by DuckDB, ETLX uses internal streaming and chunked transfers.\ntype: db_2_db\rparams:\rsource:\rconn: mssql:sqlserver://...\rsql: source_query\rchunk_size: 1000\rtarget:\rconn: postgres:...\rsql: insert_sql This allows ETLX to move data engine-to-engine using pure SQL, without intermediate files.\nODBC \u0026 Legacy Systems linkETLX integrates with:\nAny database supported by ODBC Any driver supported by sqlx This includes:\nLegacy ERP systems Excel via ODBC Proprietary databases When direct scanning is not possible, ETLX can:\nStream data Export to CSV Re-ingest into DuckDB or another engine Design Principles linkMulti-engine execution in ETLX follows these principles:\nExplicit is better than implicit SQL remains the contract between engines Engines are interchangeable, not hardcoded Metadata drives execution, not engine-specific logic Summary linkâœ” DuckDB is embedded and recommended, but optional âœ” SQLite, PostgreSQL, MySQL, MSSQL, and ODBC are supported âœ” Pipelines can run on a single engine or multiple engines âœ” ETLX adapts to enterprise and open-source environments âœ” SQL is the unifying execution layer\nETLX lets you choose the engine that fits your constraints â€” without rewriting your pipelines.\n"
            }
        );
    index.add(
            {
                id:  19 ,
                href: "\/etlxdocs\/docs\/features\/beyond-etl\/",
                title: "Beyond ETL / ELT",
                description: "ETLX as a declarative specification for reporting, document generation, exports, and regulatory workflows",
                content: "Beyond ETL / ELT linkETLX is not just an ETL / ELT engine.\nIt is a declarative specification for describing how modern data workflows should be built, executed, documented, audited, and governed â€” in a way that is transparent, portable, and self-documented.\nIn ETLX, the pipeline is the documentation.\nFrom Pipelines to Specifications linkTraditional data platforms tend to:\nHide logic inside closed-source binaries\nSpread business rules across:\nSQL files Python scripts Airflow DAGs Wiki pages Spreadsheets Separate execution, documentation, validation, and governance into disconnected systems\nETLX takes the opposite approach.\nEverything that matters is declared explicitly, in one place, and in plain text.\nETLX as a Declarative Workflow Language linkAn ETLX project describes the entire lifecycle of a data workflow:\nData ingestion (ETL / ELT) Transformations (including complex, documented queries) Data quality rules Exports and report generation Document and template rendering (Excel, HTML, Markdown, XML) File transfers and external actions Logging, observability, and audit trails All of this is expressed using:\nMarkdown structure YAML metadata SQL blocks Explicit ordering and dependencies No hidden code paths. No implicit behavior.\nBeyond ETL: Core Use Cases linkðŸ“Š Reporting \u0026 Analytics linkETLX can be used to generate:\nDaily / monthly management reports KPI summaries Aggregated dashboards (as tables or files) Regulatory extracts Using:\nEXPORTS MULTI_QUERIES Template-based Excel or text exports The same SQL used for transformation can be reused directly for reporting â€” no duplication.\nðŸ“„ Document Generation linkETLX supports structured document generation using:\nExcel templates Text-based templates (HTML, Markdown, XML, TXT) With:\nSQL as the data source Go templates + Sprig functions for rendering This enables:\nAutomated reports Data dictionaries Lineage documentation Audit and compliance documents Generated documents are deterministic, versionable, and traceable to the pipeline that produced them.\nðŸ“¦ Structured Exports \u0026 Regulatory Workflows linkMany real-world pipelines are not about analytics â€” they are about delivery:\nFiles for regulators Submissions to partners Periodic data drops Certified reports ETLX supports:\nControlled exports Repeatable file layouts Deterministic naming (e.g. YYYYMMDD) Full traceability via logs Every export step is declared, logged, and auditable.\nðŸ§ª Data Quality as a First-Class Concept linkIn ETLX, data quality is not an afterthought.\nValidation rules:\nAre declared explicitly Run as part of the pipeline Can be enforced, logged, and optionally fixed This turns quality checks into contractual guarantees, not best-effort scripts.\nThe Pipeline Is the Documentation linkA key design principle of ETLX is:\nIf it is not declared, it does not exist.\nThis means:\nNo logic hidden in binaries No undocumented transformations No tribal knowledge required to understand the pipeline By reading the ETLX configuration, you can understand:\nWhat data is loaded Where it comes from How it is transformed How it is validated Where it is exported What actions are executed What is logged and audited Without running a single line of code.\nTransparency Over Abstraction linkETLX intentionally avoids:\nMagical defaults Implicit behavior Framework-specific DSLs Instead, it favors:\nSQL you already know Explicit metadata Simple, composable blocks This makes ETLX:\nEasier to audit Easier to review Easier to reason about Easier to explain to non-engineers A Specification, Not Just a Runtime linkETLX can be:\nExecuted as a CLI Embedded as a library Integrated into CI/CD Reviewed as plain text But regardless of how it is executed, the specification remains the source of truth.\nThis allows teams to:\nTreat pipelines as code Version workflows safely Review changes via pull requests Share pipelines across teams and environments When ETLX Makes the Most Sense linkETLX shines when you need:\nEnd-to-end transparency Auditable pipelines Strong governance SQL-first workflows Deterministic outputs Minimal runtime dependencies Especially in:\nRegulated environments Data engineering teams Analytics platforms Reporting-heavy organizations Summary linkETLX goes beyond ETL by:\nTreating pipelines as living documentation Making every step explicit and declarative Unifying execution, governance, and reporting Replacing hidden binaries with readable specifications It is not just an engine.\nIt is a modern way to describe data workflows â€” as they should be built today.\n"
            }
        );
    index.add(
            {
                id:  20 ,
                href: "\/etlxdocs\/docs\/features\/api\/",
                title: "Go API \u0026 Programmatic Usage",
                description: "Use ETLX as a Go library to execute pipelines, run specific stages, or embed ETLX into your own applications.",
                content: "Go API \u0026 Programmatic Usage linkETLX is not only a CLI tool â€” it is also a Go library that you can embed directly into your own applications.\nThe official etlx binary is built using the same public Go APIs exposed by the project. This page documents those APIs in a ready-to-use way, based on how cmd/main.go constructs and executes pipelines.\nðŸ”§ CLI Internals (Important Context) linkThe ETLX CLI:\nUses Goâ€™s standard flag package Does not rely on Cobra or subcommands Internally calls the same APIs you can call from Go This means:\nðŸ‘‰ Anything the CLI can do, you can do programmatically\nðŸ“¦ Installing as a Go Dependency link go get github.com/realdatadriven/etlx ðŸ§  Core Concepts (Go Perspective) linkAt runtime, ETLX works in three phases:\nParse Markdown configuration Resolve execution scope (date, only, skip, steps, etc.) Execute pipeline + collect logs The Go API mirrors this exactly.\nðŸ— Creating an ETLX Engine link import \"github.com/realdatadriven/etlx\" engine := \u0026etlx.ETLX{} This ETLX instance holds:\nParsed pipeline configuration Execution options Runtime state Execution logs ðŸ“„ Loading a Pipeline Configuration linkLoad from file link err := engine.ConfigFromFile(\"pipeline.md\")\rif err != nil {\rpanic(err)\r} Load from in-memory Markdown linkUseful for dynamic or generated pipelines:\nmd := `# ETLX Pipeline...`\rerr := engine.ConfigFromMDText(md)\rif err != nil {\rpanic(err)\r} ðŸ“† Running a Pipeline (Equivalent to CLI) linkEquivalent CLI:\netlx --config pipeline.md --date 2025-01-01 Minimal execution link dates := []time.Time{\rtime.Date(2025, 1, 1, 0, 0, 0, 0, time.UTC),\r}\rlogs, err := engine.RunETL(dates, nil, nil)\rif err != nil {\rpanic(err)\r} ðŸŽ¯ Execution Options (--only, --skip, --steps) linkThe CLI flags are translated into a simple options map.\nRun only specific keys link opts := map[string]any{\r\"only\": []string{\"sales\", \"customers\"},\r}\rlogs, err := engine.RunETL(dates, nil, opts) Equivalent CLI:\netlx --config pipeline.md --only sales,customers Skip specific keys link opts := map[string]any{\r\"skip\": []string{\"debug_tables\"},\r} Run specific lifecycle steps link opts := map[string]any{\r\"steps\": []string{\"extract\", \"load\"},\r} Equivalent CLI:\netlx --config pipeline.md --steps extract,load ðŸ§¹ Clean \u0026 Drop Operations linkEquivalent CLI:\netlx --config pipeline.md --clean\retlx --config pipeline.md --drop Clean SQL blocks link opts := map[string]any{\r\"clean\": true,\r}\rengine.RunETL(dates, nil, opts) Drop SQL blocks link opts := map[string]any{\r\"drop\": true,\r}\rengine.RunETL(dates, nil, opts) ðŸ“Š Accessing Execution Logs linkEvery execution returns structured logs.\nlogs, _ := engine.RunETL(dates, nil, nil)\rfor _, log := range logs {\rfmt.Println(\rlog.Name,\rlog.Ref,\rlog.Duration,\rlog.Success,\rlog.Msg,\r)\r} Each log entry typically contains:\nField Description name Step or block name ref Reference date start Start timestamp end End timestamp duration Execution time success Success / failure msg Message or error ðŸ§ª Running Specific Sections linkETLX exposes explicit execution entry points for each pipeline capability.\nAll functions share the same execution model:\nfunc(dates []time.Time, cfg any, opts map[string]any) ([]etlx.Log, error) The difference is which Markdown sections are executed.\nâ–¶ï¸ RunETL linkExecutes the full ETL / ELT lifecycle see ETL / ELT\nlogs, err := engine.RunETL(dates, nil, nil) ðŸ§ª RunDATA_QUALITY linkRuns operational or auxiliary scripts see Data Quality\nlogs, err := engine.RunDATA_QUALITY(dates, nil, nil) ðŸ“¤ RunEXPORTS linkExecutes export definitions see Exports\nlogs, err := engine.RunEXPORTS(dates, nil, nil) ðŸ“œ RunSCRIPTS linkRuns operational or auxiliary scripts see Scripts\nlogs, err := engine.RunSCRIPTS(dates, nil, nil) âš™ï¸ RunACTIONS linkRuns post-processing or orchestration actions see Actions\nlogs, err := engine.RunACTIONS(dates, nil, nil) ðŸ§¾ RunLOGS linkProcesses logging or audit blocks see Logs / Observability\nlogs, err := engine.RunLOGS(dates, nil, nil) ðŸ”” RunNOTIFY linkExecutes notification definitions see Notificatios\nlogs, err := engine.RunNOTIFY(dates, nil, nil) ðŸ§µ Using ETLX Inside Long-Running Services link func runPipeline(ref time.Time) error {\rengine := \u0026etlx.ETLX{}\rif err := engine.ConfigFromFile(\"pipeline.md\"); err != nil {\rreturn err\r}\r_, err := engine.RunETL([]time.Time{ref}, nil, nil)\rreturn err\r} ðŸ§  Why This Matters linkBecause ETLX is:\nA runtime An open specification A Go library You are not locked into:\nA closed binary A hidden execution model A proprietary DSL The Markdown pipeline is the documentation, and the Go API is the execution layer.\nðŸ Summary linkâœ” CLI uses standard Go flags âœ” Go API mirrors the CLI exactly âœ” Pipelines can be executed from files or memory âœ” Each lifecycle has its own entry point âœ” Structured logs for audit \u0026 observability âœ” Ideal for embedding, automation, and regulated workloads\n"
            }
        );
    index.add(
            {
                id:  21 ,
                href: "\/etlxdocs\/docs\/advanced-examples\/",
                title: "Advanced Examples",
                description: "A collection of advanced examples demonstrating various features and capabilities.",
                content: "Welcome to the Advanced Examples section! Here, youâ€™ll find a variety of in-depth examples that showcase the advanced features and capabilities of our platform. Whether youâ€™re looking to implement complex workflows, integrate with third-party services, or optimize performance, these examples will provide you with the guidance and inspiration you need.\n"
            }
        );
    index.add(
            {
                id:  22 ,
                href: "\/etlxdocs\/docs\/advanced-examples\/embedded-sqlite\/",
                title: "Embedded SQLite Example",
                description: "Explore example of using embedded SQLite database with your process, not requiring a separate database server setup.",
                content: "Embedded SQLite Example linkThis section provides examples of using embedded SQLite databases as storage. SQLite is a self-contained, serverless, zero-configuration, transactional SQL database engine that is widely used for local data storage in applications. Good for prototyping, small to medium-sized applications, and scenarios where a full-fledged database server is not required, and change the databse maybe as simple as change the environment variable with the connection string.\nWhy use sqlite? link Simplicity: SQLite is easy to set up and requires no server configuration. Portability: The entire database is stored in a single file, making it easy to move and share, and explore data locally with tools like DB Browser for SQLite. Lightweight: SQLite has a small footprint and is efficient for read-heavy workloads. All those points make it ideal for teaching, prototyping, demos, â€¦\nExtract and Transform data linkHere you going to use sqlite as an example to store your data, and use in memory duckdb to do the heavy lifting of extract and transform the data. Its an advanced exemple where query docuementation is shown as well\n# EXTRACT_LOAD\r```yaml metadata\rname: EXTRACT_LOAD\rruns_as: ETL\rdescription: Extracts and Loads the data sets to the local analitical database\ractive: true\r```\r## TRIP_DATA\r```yaml metadata\rname: TRIP_DATA\rdescription: \"Example extrating trip data from web to a local database\"\rtable: TRIP_DATA\rload_conn: \"duckdb:\"\rload_before_sql:\r- INSTALL httpfs\r- INSTALL sqlite\r- \"ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE)\"\rload_sql: extract_load_trip_data\rload_after_sql: DETACH \"DB\"\rdrop_sql: DROP TABLE IF EXISTS \"DB\".\"\"\rclean_sql: DELETE FROM \"DB\".\"\"\rrows_sql: SELECT COUNT(*) AS \"nrows\" FROM \"DB\".\"\"\ractive: true\r```\r```sql\r-- extract_load_trip_data\rCREATE OR REPLACE TABLE \"DB\".\"\" AS\r[[QUERY_EXTRACT_TRIP_DATA]]\r```\r```sql\r-- extract_load_trip_data_with_out_doc_field_metadata\rCREATE OR REPLACE TABLE \"DB\".\"\" AS\rFROM read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet')\r```\r## ZONES\r```yaml metadata\rname: ZONES\rdescription: \"Taxi Zone Lookup Table\"\rtable: ZONES\rload_conn: \"duckdb:\"\rload_before_sql: \"ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE)\"\rload_sql: extract_load_zones\rload_after_sql: DETACH \"DB\"\rdrop_sql: DROP TABLE IF EXISTS \"DB\".\"\"\rclean_sql: DELETE FROM \"DB\".\"\"\rrows_sql: SELECT COUNT(*) AS \"nrows\" FROM \"DB\".\"\"\ractive: true\r```\r```sql\r-- extract_load_zones\rCREATE OR REPLACE TABLE \"DB\".\"\" AS\rSELECT *\rFROM 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv';\r```\r# QUERY_EXTRACT_TRIP_DATA\rThis QueryDoc extracts selected fields from the NYC Yellow Taxi Trip Record dataset.\r```yaml metadata\rname: QUERY_EXTRACT_TRIP_DATA\rdescription: \"Extracts essential NYC Yellow Taxi trip fields (with governance metadata).\"\rowner: taxi-analytics-team\rdetails: \"https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\"\rsource:\ruri: \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\rformat: parquet\r```\r## VendorID\r```yaml metadata\rname: VendorID\rdescription: \"A code indicating which TPEP provider generated the record.\r1=CMT, 2=Curb, 6=Myle, 7=Helix.\"\rtype: integer\rowner: data-providers\r```\r```sql\r-- select\rSELECT VendorID\r```\r```sql\r-- from\rFROM read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet')\r```\r## tpep_pickup_datetime\r```yaml metadata\rname: tpep_pickup_datetime\rdescription: \"Timestamp when the meter was engaged (trip start).\"\rtype: timestamp\rowner: taxi-analytics-team\r```\r```sql\r-- select\r, tpep_pickup_datetime\r```\r## tpep_dropoff_datetime\r```yaml metadata\rname: tpep_dropoff_datetime\rdescription: \"Timestamp when the meter was disengaged (trip end).\"\rtype: timestamp\rowner: taxi-analytics-team\r```\r```sql\r-- select\r, tpep_dropoff_datetime\r```\r## passenger_count\r```yaml metadata\rname: passenger_count\rdescription: \"Number of passengers, typically entered by the driver.\"\rtype: integer\rowner: ops\r```\r```sql\r-- select\r, passenger_count\r```\r## trip_distance\r```yaml metadata\rname: trip_distance\rdescription: \"Elapsed trip distance in miles, reported by the meter.\"\rtype: double\rowner: taxi-analytics-team\r```\r```sql\r-- select\r, trip_distance\r```\r## RatecodeID\r```yaml metadata\rname: RatecodeID\rdescription: \"Final rate code at trip end.\r1=Standard, 2=JFK, 3=Newark, 4=Nassau/Westchester,\r5=Negotiated fare, 6=Group ride, 99=Unknown.\"\rtype: integer\rowner: finance\r```\r```sql\r-- select\r, RatecodeID\r```\r## store_and_fwd_flag\r```yaml metadata\rname: store_and_fwd_flag\rdescription: \"'Y' if the trip record was held in the vehicle before transmission (no server connection). 'N' otherwise.\"\rtype: string\rowner: platform\r```\r```sql\r-- select\r, store_and_fwd_flag\r```\r## PULocationID\r```yaml metadata\rname: PULocationID\rdescription: \"TLC Taxi Zone ID for pickup location.\"\rtype: integer\rowner: geo\r```\r```sql\r-- select\r, PULocationID\r```\r## DOLocationID\r```yaml metadata\rname: DOLocationID\rdescription: \"TLC Taxi Zone ID for dropoff location.\"\rtype: integer\rowner: geo\r```\r```sql\r-- select\r, DOLocationID\r```\r## payment_type\r```yaml metadata\rname: payment_type\rdescription: \"How the passenger paid:\r0=Flex Fare, 1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided trip.\"\rtype: integer\rowner: finance\r```\r```sql\r-- select\r, payment_type\r```\r## fare_amount\r```yaml metadata\rname: fare_amount\rdescription: \"Time-and-distance fare calculated by the meter.\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, fare_amount\r```\r## extra\r```yaml metadata\rname: extra\rdescription: \"Miscellaneous extras and surcharges (e.g., peak surcharge).\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, extra\r```\r## mta_tax\r```yaml metadata\rname: mta_tax\rdescription: \"0.50 USD MTA tax triggered by metered rate.\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, mta_tax\r```\r## tip_amount\r```yaml metadata\rname: tip_amount\rdescription: \"Tip in USD. Only includes credit-card tips; cash tips are not recorded.\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, tip_amount\r```\r## tolls_amount\r```yaml metadata\rname: tolls_amount\rdescription: \"Total tolls paid for the trip.\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, tolls_amount\r```\r## improvement_surcharge\r```yaml metadata\rname: improvement_surcharge\rdescription: \"Flat surcharge added at flag drop. Introduced in 2015.\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, improvement_surcharge\r```\r## total_amount\r```yaml metadata\rname: total_amount\rdescription: \"Total charged amount (fare + extras + taxes + tips).\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, total_amount\r```\r## congestion_surcharge\r```yaml metadata\rname: congestion_surcharge\rdescription: \"NY State congestion surcharge assessed per trip.\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, congestion_surcharge\r```\r## airport_fee\r```yaml metadata\rname: airport_fee\rdescription: \"Fee for pickups at JFK or LaGuardia airports.\"\rtype: numeric\rowner: finance\r```\r```sql\r-- select\r, airport_fee\r```\r## cbd_congestion_fee\r```yaml metadata\rname: cbd_congestion_fee\rdescription: \"MTA Congestion Relief Zone fee (in effect after Jan 5, 2025).\"\rtype: numeric\rowner: finance\ractive: false\r```\r```sql\r-- select\r, cbd_congestion_fee\r```\r# TRANSFORM\r```yaml metadata\rname: TRANSFORM\rruns_as: ETL\rdescription: Transforms the inputs into to desrable outputs\rconnection: \"sqlite3:sqlite_ex.db\"\rdatabase: \"sqlite3:sqlite_ex.db\"\ractive: true\r```\r## MostPopularRoutes\r```yaml metadata\rname: MostPopularRoutes\rdescription: |\rMost Popular Routes - Identify the most common pickup-dropoff route combinations to understand travel patterns.\rtable: MostPopularRoutes\rtransform_conn: \"duckdb:\"\rtransform_before_sql: \"ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE)\"\rtransform_sql: trf_most_popular_routes\rtransform_after_sql: DETACH \"DB\"\rdrop_sql: DROP TABLE IF EXISTS \"DB\".\"\"\rclean_sql: DELETE FROM \"DB\".\"\"\rrows_sql: SELECT COUNT(*) AS \"nrows\" FROM \"DB\".\"\"\ractive: true\r```\r```sql\r-- trf_most_popular_routes\rCREATE OR REPLACE TABLE \"DB\".\"\" AS\r[[QUERY_TOP_ZONES]]\rLIMIT 15\r```\r# QUERY_TOP_ZONES\r```yaml metadata\rname: QUERY_TOP_ZONES\rdescription: \"Most common pickup/dropoff zone combinations with aggregated metrics.\"\rowner: taxi-analytics-team\rsource:\r- TRIP_DATA\r- ZONES\r```\r## pickup_borough\r```yaml metadata\rname: pickup_borough\rdescription: \"Borough of the pickup location (from ZONES lookup).\"\rtype: string\rderived_from:\r- TRIP_DATA.PULocationID\r- ZONES.Borough\rformula: |\rThis field is a direct lookup of the pickup borough using the pickup location ID.\r```\r```sql\r-- select\rSELECT zpu.Borough AS pickup_borough\r```\r```sql\r-- from\rFROM DB.TRIP_DATA AS t\r```\r```sql\r-- join\rJOIN DB.ZONES AS zpu ON t.PULocationID = zpu.LocationID\r```\r```sql\r-- group_by\rGROUP BY pickup_borough\r```\r## pickup_zone\r```yaml metadata\rname: pickup_zone\rdescription: \"Zone name of the pickup location (from ZONES lookup).\"\rtype: string\rderived_from:\r- TRIP_DATA.PULocationID\r- ZONES.Zone\rformula: |\rThis field is a direct lookup of the pickup zone using the pickup location ID.\r```\r```sql\r-- select\r, zpu.Zone AS pickup_zone\r```\r```sql\r-- group_by\r, pickup_zone\r```\r## dropoff_borough\r```yaml metadata\rname: dropoff_borough\rdescription: \"Borough of the dropoff location (from ZONES lookup).\"\rtype: string\rderived_from:\r- TRIP_DATA.DOLocationID\r- ZONES.Borough\rformula: |\rThis field is a direct lookup of the dropoff borough using the dropoff location ID.\r```\r```sql\r-- select\r, zdo.Borough AS dropoff_borough\r```\r```sql\r-- join\rJOIN DB.ZONES AS zdo ON t.DOLocationID = zdo.LocationID\r```\r```sql\r-- group_by\r, dropoff_borough\r```\r## dropoff_zone\r```yaml metadata\rname: dropoff_zone\rdescription: \"Zone name of the dropoff location (from ZONES lookup).\"\rtype: string\rderived_from:\r- TRIP_DATA.DOLocationID\r- ZONES.Zone\rformula: |\rThis field is a direct lookup of the dropoff zone using the dropoff location ID.\r```\r```sql\r-- select\r, zdo.Zone AS dropoff_zone\r```\r```sql\r-- group_by\r, dropoff_zone\r```\r## total_trips\r```yaml metadata\rname: total_trips\rdescription: \"Total number of trips between each pickup/dropoff zone pair.\"\rtype: integer\rderived_from:\r- TRIP_DATA.*\rformula: | \"\rCount the number of trips in each pickup/dropoff combination.\r**Formula (LaTeX):**\r```latex\r\\text{total\\_trips} = \\sum_{i=1}^{N} 1\r```\"\r```\r```sql\r-- select\r, COUNT(*) AS total_trips\r```\r```sql\r-- order_by\rORDER BY total_trips DESC\r```\r## avg_fare\r```yaml metadata\rname: avg_fare\rdescription: \"Average total fare for trips between the selected pickup and dropoff zones.\"\rtype: numeric\rderived_from:\r- TRIP_DATA.total_amount\rformula: | \"\rCompute the arithmetic mean of total fares for the group, rounded to 2 decimals.\r**Formula (LaTeX):**\r```latex\r\\text{avg\\_fare} = \\text{round}\\!\\left(\\frac{1}{N}\\sum_{i=1}^{N} \\text{total\\_amount}_i,\\ 2\\right)\r```\"\r```\r```sql\r-- select\r, ROUND(AVG(t.total_amount), 2) AS avg_fare\r```\r## avg_distance\r```yaml metadata\rname: avg_distance\rdescription: \"Average trip distance (miles).\"\rtype: numeric\rderived_from:\r- TRIP_DATA.trip_distance\rformula: | \"\rCompute the arithmetic mean of trip distances for the group, rounded to 2 decimals.\r**Formula (LaTeX):**\r```latex\r\\text{avg\\_distance} = \\text{round}\\!\\left(\\frac{1}{N}\\sum_{i=1}^{N} \\text{trip\\_distance}_i,\\ 2\\right)\r```\"\r```\r```sql\r-- select\r, ROUND(AVG(t.trip_distance), 2) AS avg_distance\r```\r# QUALITY_CHECK\r```yaml\rdescription: \"Runs some queries to check quality / validate.\"\rruns_as: DATA_QUALITY\ractive: true\r```\r## Rule0001\r```yaml\rname: Rule0001\rdescription: \"Check if the payment_type has only the values 0=Flex Fare, 1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided trip.\"\rconnection: \"duckdb:\"\rbefore_sql: \"ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE)\"\rquery: quality_check_query\rfix_quality_err: fix_quality_err_query\rcolumn: total_err # Defaults to 'total'.\rcheck_only: false # runs only quality check if true\rfix_only: false # runs only quality fix if true and available and possible\rafter_sql: \"DETACH DB\"\ractive: true\r```\r```sql\r-- quality_check_query\rSELECT COUNT(*) AS \"total_err\"\rFROM \"TRIP_DATA\"\rWHERE \"payment_type\" NOT IN (0,1,2,3,4,5,6);\r```\r```sql\r-- fix_quality_err_query\rUPDATE \"TRIP_DATA\"\rSET \"payment_type\" = 'default value'\rWHERE \"payment_type\" NOT IN (0,1,2,3,4,5,6);\r```\r## Rule0002\r```yaml\rname: Rule0002\rdescription: \"Check if there is any trip with distance less than or equal to zero.\"\rconnection: \"duckdb:\"\rbefore_sql: \"ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE)\"\rquery: quality_check_query\rfix_quality_err: null # no automated fixing for this\rcolumn: total_err # Defaults to 'total'.\rafter_sql: \"DETACH DB\"\ractive: true\r```\r```sql\r-- quality_check_query\rSELECT COUNT(*) AS \"total_err\"\rFROM \"TRIP_DATA\"\rWHERE NOT \"trip_distance\" \u003e 0;\r```\r# SAVE_LOGS\r```yaml metadata\rname: SAVE_LOGS\rruns_as: LOGS\rdescription: Saving the logs in the same DB instead of the deafult temp style\rtable: etlx_logs\rconnection: \"duckdb:\"\rbefore_sql:\r- \"ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE)\"\r- \"USE DB;\"\r- INSTALL json\r- LOAD json\r- \"get_dyn_queries[create_columns_missing](ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE), DETACH DB)\"\rsave_log_sql: INSERT INTO \"DB\".\"\" BY NAME FROM read_json('')\rsave_on_err_patt: \"(?i)table.+does.+not.+exist\"\rsave_on_err_sql: CREATE TABLE IF NOT EXISTS \"DB\".\"\" AS FROM read_json('');\rafter_sql:\r- \"USE memory;\"\r- DETACH \"DB\"\ractive: true\r```\r```sql\r-- create_columns_missing\rWITH source_columns AS (\rSELECT column_name, column_type\rFROM (DESCRIBE SELECT * FROM read_json(''))\r),\rdestination_columns AS (\rSELECT column_name, data_type as column_type\rFROM duckdb_columns\rWHERE table_name = ''\r),\rmissing_columns AS (\rSELECT s.column_name, s.column_type\rFROM source_columns s\rLEFT JOIN destination_columns d ON s.column_name = d.column_name\rWHERE d.column_name IS NULL\r)\rSELECT 'ALTER TABLE \"DB\".\"\" ADD COLUMN \"' || column_name || '\" ' || column_type || ';' AS query\rFROM missing_columns\rWHERE (SELECT COUNT(*) FROM destination_columns) \u003e 0;\r``` The ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE) that repeates troughout the code is what connects to the embedded sqlite database file named sqlite_ex.db. You can change this filename as needed. Also you can add it to an environment variable and reference it that way for more flexibility, by replacing it with @ENV_VAR_NAME\n"
            }
        );
    index.add(
            {
                id:  23 ,
                href: "\/etlxdocs\/docs\/advanced-examples\/governance-artifacts\/",
                title: "Governance Artifacts",
                description: "Generating governance artifacts like data dictionaries and data quality rules from ETLX pipelines.",
                content: "Generate Governance Artifacts linkTo generate governance artifacts for the example above, you can add an EXPORTS block that defines a text template. This template can traverse all the metadata already embedded in the pipeline to generate artifacts such as a data dictionary, data quality rules, or compliance documentation.\nThis is possible because an export template in ETLX is not limited to query results. It can generate text from any data passed through it via data_sql, including the pipeline configuration itself, which is available through the .conf variable.\nAs mentioned earlier, the Markdown specification is parsed into a nested Go map[string]any, where every structural and semantic aspect of the documentâ€”queries, fields, metadata, ordering, and relationshipsâ€”is preserved as data. This means the specification is not just documentation, but a first-class data structure that can be queried, transformed, and rendered.\nIn practice, this allows the pipeline to produce its own governance artifacts directly from the same source of truth, ensuring that documentation, execution, and governance always remain aligned.\nFollowing the example Embedded SQLite, here is how you could define an EXPORTS block to generate a data dictionary as an HTML file:\n# GOVERNANCE_ARTIFACTS ```yaml metadata name: GOVERNANCE_ARTIFACTS description: \"Generates governance artifacts like data dictionary and data quality rules.\" runs_as: EXPORTS active: true ``` ## DATA_DICTIONARY ```yaml metadata name: DATA_DICTIONARY description: \"Generates a data dictionary from the metadata of the queries.\" connection: \"duckdb:\" before_sql: \"ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE)\" data_sql: null after_sql: \"DETACH DB\" tmp_prefix: null text_template: true template: data_dictionary_template return_content: false path: \"data_dictionary.html\" active: true ``` ```html data_dictionary_template Governance Artifacts Governance Artifacts {{- range $blockName, $block := .conf }} {{- if and (ne $blockName \"__order\") (ne $blockName \"metadata\") (kindIs \"map\" $block) }} {{- $meta := index $block \"metadata\" }} {{- if or (not $meta) (not (hasKey $meta \"run_as\")) }} {{ $blockName }} {{- with $meta }} â€” {{ index . \"description\" }}{{ end }} {{- $order := index $block \"__order\" }} {{- if kindIs \"slice\" $order }} Data Dictionary Field Name Description Type Owner Derived From Formula {{- range $i, $fieldName := $order }} {{- $field := index $block $fieldName }} {{- if kindIs \"map\" $field }} {{- $fmeta := index $field \"metadata\" }} {{ $fieldName }} {{ index $fmeta \"description\" | default \"N/A\" }} {{ index $fmeta \"type\" | default \"N/A\" }} {{ index $fmeta \"owner\" | default \"N/A\" }} {{- with index $fmeta \"derived_from\" }} {{- range $j, $src := . }} {{- if $j }}, {{ end }}{{ $src }} {{- end }} {{- else }} N/A {{- end }} {{ index $fmeta \"formula\" | default \"N/A\" }} {{- end }} {{- end }} {{- end }} {{- end }} {{- if and $meta (eq (index $meta \"run_as\") \"DATA_QUALITY\") }} {{ $blockName }} {{- with $meta }} â€” {{ index . \"description\" }}{{ end }} Data Quality Rules Rule Name Description Active {{- range $ruleName, $rule := $block }} {{- if and (ne $ruleName \"metadata\") (ne $ruleName \"__order\") (kindIs \"map\" $rule) }} {{- $rmeta := index $rule \"metadata\" }} {{ $ruleName }} {{ index $rmeta \"description\" | default \"N/A\" }} {{- if hasKey $rule \"active\" -}} {{ index $rule \"active\" }} {{- else -}} true {{- end -}} {{- end }} {{- end }} {{- end }} {{- end }} {{- end }} ``` Hereâ€™s a concrete layout example of what your template would generate when rendered, using a fictional ETLX pipeline. This is not the template, but the resulting page structure/content a user would see in the browser.\nData Dictionary linksales_orders â€” Cleaned and enriched sales orders dataset link Field Name Description Type Owner Derived From Formula order_id Unique identifier for the order STRING sales_team raw_orders.order_id N/A customer_id Identifier of the customer STRING crm_team raw_orders.customer_ref N/A order_date Date when the order was placed DATE sales_team raw_orders.created_at CAST(created_at AS DATE) total_amount Total order value after tax DECIMAL finance_team line_items.amount, tax.amount SUM(amount) + SUM(tax) is_high_value Flags high value orders BOOLEAN analytics total_amount total_amount \u003e 1000 daily_sales_summary â€” Aggregated daily sales metrics link Field Name Description Type Owner Derived From Formula sales_date Business date DATE analytics sales_orders.order_date N/A total_orders Number of orders per day INTEGER analytics sales_orders.order_id COUNT(order_id) gross_revenue Total revenue before refunds DECIMAL finance_team sales_orders.total_amount SUM(total_amount) avg_order_value Average value per order DECIMAL analytics gross_revenue, total_orders gross_revenue / total_orders Notes on how this maps to your template\nOnly pipelines without metadata.run_as are shown âœ”ï¸ Fields are rendered in __order sequence âœ”ï¸ Keys like __order are ignored as fields âœ”ï¸ Only map-based field definitions are rendered (slices ignored) âœ”ï¸ Missing metadata gracefully falls back to N/A âœ”ï¸ derived_from supports multiple sources âœ”ï¸ Customizing the Data Dictionary Output linkETLX allows you to fully customize how a Data Dictionary is generated and rendered. The example shown above is rendered as an HTML table, but the same metadata can be presented in many different ways depending on your needs.\nConditional Rendering linkYou can apply conditions to control what is included or excluded in the output.\nTypical use cases include:\nRendering only pipelines without metadata.run_as Skipping technical or internal fields Showing optional columns only when metadata exists Applying different layouts for different environments (e.g. dev vs prod) Example conditions:\nInclude a field only if metadata exists Exclude system keys such as __order Render only map-based field definitions (ignore slices or arrays) This makes the documentation data-driven, consistent, and safe to auto-generate.\nStyling and Presentation linkBecause the rendering layer is template-based, you can control visual styling independently from logic.\nWith HTML templates you can:\nApply custom CSS (fonts, colors, spacing) Highlight derived or calculated fields Emphasize business-critical columns Add section headers, notes, or warnings For example:\nCalculated fields could be highlighted Fields owned by Finance could have a different style Deprecated fields could be visually marked Alternative Layouts linkWhile the example uses a tabular layout, ETLX does not enforce a specific structure.\nYou may render the same metadata as:\nBullet-point documentation link â€¢ order_id (STRING) - Description: Unique identifier for the order - Owner: sales_team - Source: raw_orders.order_id â€¢ total_amount (DECIMAL) - Derived from: line_items.amount, tax.amount - Formula: SUM(amount) + SUM(tax) This format is often easier to read in Markdown or plain-text documentation.\nMultiple Output Formats linkAlthough this example uses HTML, the same metadata can be rendered into other formats using different templates:\nHTML â†’ Web documentation, internal portals CSV â†’ Audits, governance, Excel exports XML â†’ System integrations, legacy tools Markdown â†’ GitHub, GitLab, static docs JSON â†’ APIs, automation, lineage tools The data dictionary generation logic remains the same â€” only the template changes.\nWhy This Matters in ETLX linkThis approach ensures that:\nDocumentation stays in sync with pipelines Business and technical users share a single source of truth Governance, lineage, and ownership are first-class citizens Output can evolve without touching the ETL logic In ETLX, documentation is not an afterthought â€” it is generated, versioned, and reproducible.\n"
            }
        );
    index.add(
            {
                id:  24 ,
                href: "\/etlxdocs\/docs\/advanced-examples\/load-external-dependencies\/",
                title: "Loading External Dependencies in ETLX",
                description: "How to split large ETLX pipelines into reusable, composable external files.",
                content: "Loading External Dependencies in ETLX linkIn real-world scenarios, an ETLX project can quickly grow beyond what is practical to manage in a single file. As pipelines evolve, you may also want to reuse queries, transformations, or documentation blocks across multiple projects.\nTo address this, ETLX allows you to split your pipeline into multiple files and load them as external dependencies using the require feature.\nA good example of this pattern is the Embedded SQLite walkthrough. While the core execution logic is relatively small, with just two inputs and one transformation, the pipeline grows rapidly once queries are documented and detailed at field level.\nInstead of letting a single file become too large, you can move queries (or even other blocks) into dedicated files and load them into your main pipeline, like this:\nqueries/ â”œâ”€â”€ trip_data.md â”œâ”€â”€ top-zones.md pipeline-sqlite.md This approach keeps pipelines modular, readable, and reusable, without sacrificing documentation or governance.\nMoving QUERY_EXTRACT_TRIP_DATA to an External File linkFirst, we move the # QUERY_EXTRACT_TRIP_DATA block into a separate file named trip_data.md inside a queries folder.\nThis file contains:\nThe query definition Column-level metadata SQL fragments Documentation and governance details # QUERY_EXTRACT_TRIP_DATA This QueryDoc extracts selected fields from the NYC Yellow Taxi Trip Record dataset. ```yaml metadata name: QUERY_EXTRACT_TRIP_DATA description: \"Extracts essential NYC Yellow Taxi trip fields (with governance metadata).\" owner: taxi-analytics-team details: \"https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\" source: uri: \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\" format: parquet ``` ## VendorID ```yaml metadata name: VendorID description: \"A code indicating which TPEP provider generated the record. 1=CMT, 2=Curb, 6=Myle, 7=Helix.\" type: integer owner: data-providers ``` ```sql -- select SELECT VendorID ``` ```sql -- from FROM read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet') ``` ## tpep_pickup_datetime ```yaml metadata name: tpep_pickup_datetime description: \"Timestamp when the meter was engaged (trip start).\" type: timestamp owner: taxi-analytics-team ``` ```sql -- select , tpep_pickup_datetime ``` ... (remaining SQL fragments) ðŸ’¡ Note Keeping metadata next to SQL ensures that documentation, lineage, and ownership are always versioned together with the logic that produces the data.\nMoving QUERY_TOP_ZONES to an External File linkNext, we extract the # QUERY_TOP_ZONES block into another file called top-zones.md, located in the same queries directory.\nThis query demonstrates:\nJoins across datasets Aggregations Derived metrics Rich field-level metadata with formulas # QUERY_TOP_ZONES ```yaml metadata name: QUERY_TOP_ZONES description: \"Most common pickup/dropoff zone combinations with aggregated metrics.\" owner: taxi-analytics-team source: - TRIP_DATA - ZONES ``` ## pickup_borough ```yaml metadata name: pickup_borough description: \"Borough of the pickup location (from ZONES lookup).\" type: string derived_from: - TRIP_DATA.PULocationID - ZONES.Borough formula: | This field is a direct lookup of the pickup borough using the pickup location ID. ``` ```sql -- select SELECT zpu.Borough AS pickup_borough ``` ```sql -- from FROM DB.TRIP_DATA AS t ``` ```sql -- join JOIN DB.ZONES AS zpu ON t.PULocationID = zpu.LocationID ``` ```sql -- group_by GROUP BY pickup_borough ``` ... (remaining SQL fragments) This file remains fully self-contained, readable, and executable once loaded into a pipeline.\nUsing External Dependencies in the Main Pipeline linkFinally, we update the main pipeline-sqlite.md file to load these external query definitions using the require mechanism.\n... # LOAD_DEPENDENCIES ```yaml metadata name: LOAD_DEPENDENCIES description: \"Load external query dependencies for the ETLX pipeline.\" run_as: REQUIRE ``` ## LOAD_TRIP_DATA_QUERY ```yaml metadata name: LOAD_TRIP_DATA_QUERY description: \"Load QUERY_EXTRACT_TRIP_DATA from an external file.\" path: \"queries/trip_data.md\" ``` ## LOAD_TOP_ZONES_QUERY ```yaml metadata name: LOAD_TOP_ZONES_QUERY description: \"Load QUERY_TOP_ZONES from an external file.\" path: \"queries/top-zones.md\" ``` Once loaded, the blocks # QUERY_EXTRACT_TRIP_DATA and # QUERY_TOP_ZONES are available to the pipeline exactly as if they were defined in the same file.\nWhy This Matters linkThis modular approach allows you to:\nKeep pipelines small and readable Reuse queries across projects Share standardized transformations Version logic and documentation together Scale ETLX projects without losing clarity In ETLX, external dependencies are not just includes â€” they are first-class, documented, executable building blocks.\n"
            }
        );
    index.add(
            {
                id:  25 ,
                href: "\/etlxdocs\/docs\/advanced-examples\/schema-update\/",
                title: "Schema Update Example",
                description: "An example demonstrating how to safely evolve a target table schema when source schema / data changes.",
                content: "Schema Update Example linkThere are many scenarios where you load all columns from a source dataset into a target table and expect the schema to remain stable over time.\nIn practice, this assumption often breaks.\nA common failure mode is:\nThe source dataset introduces new columns Your pipeline still tries to insert into the existing table The workflow fails due to a schema mismatch This is a perfect use case for dynamic schema handling.\nETLX provides this capability via Dynamic Query Generation, allowing you to compare source and target schemas at runtime and generate the required SQL automatically.\nUse Case: Monthly Incremental Loads with Schema Evolution linkFollowing the Embedded SQLite example, imagine the following workflow:\nEvery month you ingest a new file: yellow_tripdata_{YYYY-MM}.parquet You append new data instead of replacing the table You must avoid duplicate monthly loads The schema may evolve over time To support this safely, we need to:\nValidate that the month has not already been loaded Detect new columns in the source Update the target table schema if necessary Append the new data Updated TRIP_DATA Load Definition linkBelow is an updated ## TRIP_DATA block that implements this logic.\n... ## TRIP_DATA ```yaml name: TRIP_DATA description: \"Example extracting trip data from the web into a local database\" table: TRIP_DATA load_conn: \"duckdb:\" load_before_sql: \"ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE)\" # Because we are appending data, validate that the reference month # does not already exist to prevent duplicate loads load_validation: - type: throw_if_not_empty sql: FROM \"DB\".\"TRIP_DATA\" WHERE \"ref_date\" = '{YYYY-MM}' LIMIT 10 msg: \"This date is already imported â€” aborting to avoid duplicates.\" # Load strategy: # 1. Dynamically generate ALTER TABLE statements for missing columns # 2. Append new data into the table load_sql: - \"get_dyn_queries[create_missing_columns](ATTACH 'sqlite_ex.db' AS DB (TYPE SQLITE), DETACH DB)\" - update_trip_data_schema # If the table does not exist, create it instead load_on_err_match_patt: '(?i)table.+with.+name.+(\\w+).+does.+not.+exist' load_on_err_match_sql: create_trip_data_table load_after_sql: DETACH \"DB\" # difine the file path / url that can fill the placeholder | file: \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{YYYY-MM}.parquet\" active: true ``` Append Data into an Existing Table linkIf the table exists and the schema is compatible, data is appended using:\n-- update_trip_data_schema INSERT INTO \"DB\".\"\" BY NAME SELECT *, '{YYYY-MM}' AS \"ref_date\" FROM READ_PARQUET('') Using BY NAME ensures that:\nColumns are matched by name Column order changes do not break the load Newly added columns are handled safely Create the Table If It Does Not Exist linkIf the table does not yet exist, it is created directly from the source:\n-- create_trip_data_table CREATE TABLE \"DB\".\"\" AS SELECT *, '{YYYY-MM}' AS \"ref_date\" FROM READ_PARQUET('') Dynamically Generate Missing Columns linkThe following query detects columns present in the source file but missing in the target table and generates the required ALTER TABLE statements.\n-- create_missing_columns WITH source_columns AS ( -- Columns from the source parquet file SELECT \"column_name\", \"column_type\" FROM (DESCRIBE SELECT * FROM READ_PARQUET('')) ), destination_columns AS ( -- Columns from the destination table SELECT \"column_name\", \"data_type\" AS \"column_type\" FROM \"duckdb_columns\" WHERE \"table_name\" = '' ), missing_columns AS ( -- Columns present in source but missing in destination SELECT s.\"column_name\", s.\"column_type\" FROM source_columns s LEFT JOIN destination_columns d ON s.\"column_name\" = d.\"column_name\" WHERE d.\"column_name\" IS NULL ) SELECT 'ALTER TABLE \"DB\".\"\" ADD COLUMN \"' || \"column_name\" || '\" ' || \"column_type\" || ';' AS \"query\" FROM missing_columns WHERE (SELECT COUNT(*) FROM destination_columns) \u003e 0; These generated statements are executed automatically by ETLX before the append step.\nWhy This Pattern Matters linkThis is an advanced but highly practical pattern that enables:\nIncremental loads Schema evolution without manual intervention Duplicate protection Declarative, self-healing pipelines Rather than failing when the schema changes, the pipeline adapts safely and transparently.\nIn ETLX, schema evolution is not an exception â€” it is an explicit, observable part of the pipeline.\n"
            }
        );
    index.add(
            {
                id:  26 ,
                href: "\/etlxdocs\/docs\/contributing\/",
                title: "Contributing",
                description: "How to contribute to Lotus Labs.",
                content: ""
            }
        );
    index.add(
            {
                id:  27 ,
                href: "\/etlxdocs\/docs\/contributing\/how-to-contribute\/",
                title: "How to Contribute",
                description: "Contribute to code, improve documentation, help others, submit to showcase, and contribute financially.",
                content: "Contribute to code linkCreate a Pull Request link Follow the GitHub flow. Follow the Conventional Commits Specification Improve documentation link Follow the GitHub flow. Follow the Conventional Commits Specification Create an issue link Bug report Feature request Help others linkETLX Discussions is the place to get help and help others with ETLX.\nSubmit to showcase linkShare what youâ€™ve built with us.\nContribute financially linkHelp support the team developing ETLX by becoming a financial contributor.\n"
            }
        );
    index.add(
            {
                id:  28 ,
                href: "\/etlxdocs\/docs\/contributing\/code-of-conduct\/",
                title: "Code of Conduct",
                description: "Contributor Covenant Code of Conduct.",
                content: "Our Pledge linkWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\nOur Standards linkExamples of behaviour that contributes to a positive environment for our community include:\nDemonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behaviour include:\nThe use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing othersâ€™ private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities linkCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behaviour and will take appropriate and fair corrective action in response to any behaviour that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\nScope linkThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nEnforcement linkInstances of abusive, harassing, or otherwise unacceptable behaviour may be reported by contacting the project team at realdatadriven@gmail.com. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\nEnforcement Guidelines linkCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n1. Correction linkCommunity Impact: Use of inappropriate language or other behaviour deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behaviour was inappropriate. A public apology may be requested.\n2. Warning linkCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behaviour. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n3. Temporary Ban linkCommunity Impact: A serious violation of community standards, including sustained inappropriate behaviour.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n4. Permanent Ban linkCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behaviour, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\nAttribution linkThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozillaâ€™s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.\n"
            }
        );
    index.add(
            {
                id:  29 ,
                href: "\/etlxdocs\/docs\/help\/",
                title: "Help",
                description: "Help using the ETLX.",
                content: ""
            }
        );
    index.add(
            {
                id:  30 ,
                href: "\/etlxdocs\/docs\/help\/troubleshooting\/",
                title: "Troubleshooting",
                description: "Solutions to common problems.",
                content: " warning Caution - This documentation is in progress Troubleshooting linkThis section provides solutions to common issues you may encounter while using ETLX.\nBecause ETLX is declarative, SQL-first, and configuration-driven, most problems are not runtime crashes but execution decisions: blocks not running, steps being skipped, or conditions preventing execution.\nImportant mental model:\nIf ETLX executed anything, it was logged.\nIf it did nothing, the reason is almost always visible in logs or terminal output.\nIf you donâ€™t find a solution here, please open an issue or start a discussion.\nCommon Issues and Solutions linkIssue 1: You execute a pipeline, but nothing happens linkSymptoms\nThe command finishes immediately No errors are shown No data is created or modified Possible Causes \u0026 Solutions\n1ï¸âƒ£ All blocks or steps are inactive linkETLX only executes blocks and steps marked as active: true.\nIf active is missing â†’ it is considered active by default If active: false â†’ the block (and all children) are skipped Check\nactive: true If a parent block is inactive, all nested blocks are ignored.\n2ï¸âƒ£ The pipeline does not match the execution mode linkSome blocks require a specific runs_as mode (e.g. ETL, QUERY_DOC, EXPORTS, SCRIPTS).\nIf you run:\netlx --config pipeline.md But the top-level block has:\nruns_as: ETL Then ETLX may skip execution depending on flags and configuration.\nâœ” Ensure the runs_as value matches your intent and the execution context.\n3ï¸âƒ£ No executable SQL was defined linkETLX executes explicit SQL references such as:\nload_sql query_sql export_sql script_sql If a step only contains metadata and no executable SQL, it will be skipped by design.\nThis is intentional â€” metadata alone is not execution.\n4ï¸âƒ£ Conditions prevented execution linkSome steps may include conditional execution (e.g. validations or _condition).\nIf a condition evaluates to false, the step will not run.\nâœ” Check validation rules and condition SQL. âœ” The evaluation result is logged.\nIssue 2: You canâ€™t find the logs for debugging linkSymptoms\nNo visible logs in the terminal You expect SQL-level output but see nothing How logging works in ETLX (important) linkâœ” Everything is logged âœ” Logs are generated for:\nExecution steps SQL execution Errors Validation results Skipped steps and reasons If logs are not explicitly configured to be stored somewhere, ETLX will:\nðŸ‘‰ Save them to the operating systemâ€™s default temporary directory\nThis guarantees logs always exist, even if you did not define a logging sink.\n1ï¸âƒ£ Default logging behavior linkDepending on configuration, logs may be:\nPrinted to the terminal (stdout) Stored in the OS temp directory Written to files Persisted in a database Exported as structured artifacts If you did not configure a destination, check:\nThe terminal output where ETLX was executed The OS temporary folder 2ï¸âƒ£ The terminal output matters linkETLX is explicit by design.\nThe terminal where execution happens often tells you in in debugg mode ETLX_DEBUG_QUERY=true:\nWhich blocks were detected Which steps were skipped Why something didnâ€™t run Which SQL failed ðŸ‘‰ Always read the terminal output first.\n3ï¸âƒ£ Use EXPORTS for observability linkETLX encourages observable pipelines.\nYou can define an EXPORTS block to generate:\nExecution summaries Logs in tabular form Validation results Data quality reports Audit trails Logs are not only runtime artifacts â€” they can be persisted, queried, and audited like any other dataset.\nIssue 3: A step is silently skipped linkSymptoms\nSome steps run, others donâ€™t No errors are raised Possible Causes\nactive: false on the step Parent block inactive Validation rule failed Condition evaluated to false Tip\nThink of ETLX as a declarative execution graph:\nIf the configuration says â€œdonâ€™t runâ€, ETLX wonâ€™t â€” and it will log why.\nIssue 4: SQL runs fine in my database, but fails in ETLX linkSymptoms\nSQL works in psql / mysql / SSMS Fails or behaves differently in ETLX Possible Causes\n1ï¸âƒ£ Different execution engine linkETLX may execute SQL via:\nDuckDB Another attached engine Some SQL syntax, functions, or data types are engine-specific.\nâœ” Verify:\nSQL dialect Extensions Function availability 2ï¸âƒ£ Missing ATTACH or extension linkIf your SQL references external data:\nFROM SRC.my_table Make sure the source is attached:\nbefore_sql: - ATTACH 'postgres:@PG_DSN' AS SRC (TYPE POSTGRES) If itâ€™s not attached, ETLX cannot resolve it â€” and this will be logged.\nIssue 5: Data is duplicated or partially loaded linkSymptoms\nRe-running a pipeline duplicates data Unexpected extra rows Explanation\nETLX does not assume idempotency by default.\nYou must explicitly define:\nValidation rules Conditional execution Cleanup logic Best Practices\nUse load_validation Use date-based or watermark checks Fail fast if data already exists Issue 6: Exports or templates generate empty output linkSymptoms\nExport file exists but is empty Template renders without content Possible Causes\ndata_sql returned no rows Template logic does not match the data structure You are iterating over a key that does not exist Tip\nETLX templates can consume any data, including the parsed configuration itself (.conf).\nIf something exists in the config, it can be exported.\nIssue 7: I expected documentation or governance artifacts, but nothing was generated linkExplanation\nETLX only generates artifacts that are explicitly declared.\nDocumentation, data dictionaries, and audits are pipeline outputs, not side effects.\nâœ” Ensure you have an EXPORTS block that:\nReferences metadata Uses a text or HTML template Targets a file or destination General Debugging Tips link Read the terminal output Assume logs exist â€” find where they are stored Start simple: one block, one step Verify active flags Check execution order Inspect parsed configuration via exports Prefer explicit behavior over assumptions Still stuck? linkIf the issue persists:\nOpen an issue on GitHub ðŸ‘‰ https://github.com/realdatadriven/etlx/issues\nInclude:\nMinimal pipeline example Expected vs actual behavior Execution engine and version Relevant logs or terminal output ETLX favors transparency and clarity â€” most issues can be understood by inspecting configuration, logs, and execution output.\n"
            }
        );
    index.add(
            {
                id:  31 ,
                href: "\/etlxdocs\/docs\/help\/faqs\/",
                title: "FAQ",
                description: "Answers to frequently asked questions.",
                content: "General Questions linkWhat is ETLX? linkETLX is an open-source, SQL-first data workflow engine and an evolving open specification for describing complete data workflows as executable documentation.\nETLX lets you define ETL, analytics, reporting, exports, etc using Markdown + YAML + SQL, where the pipeline itself becomes the documentation, governance artifact, and execution plan.\nWhat problem does ETLX solve? linkETLX addresses common problems in modern data workflows:\nHidden logic spread across code, SQL, and orchestration tools Poor documentation and weak data governance Tight coupling between pipelines and execution engines Difficult auditing, reproducibility, and compliance ETLX makes all logic explicit, self-documenting, and auditable by design.\nWho is ETLX for? linkETLX is designed for:\nData engineers building ETL / ELT pipelines Analytics engineers working SQL-first Data scientists needing reproducible / automated data workflows Data analysts creating reports and dashboards Platform and data architects Governance, compliance, and data quality teams Organizations that value transparency and reproducibility If you believe SQL should be the transformation language and documentation should not be an afterthought, ETLX is for you.\nHow do I get started? linkStart with the documentation:\nðŸ‘‰ Getting Started \u0026 Concepts https://realdatadriven.github.io/etlxdocs\nYou can:\nRun ETLX via the CLI Embed it as a Go library Start with a single Markdown file and grow from there Technical Questions linkWhat technologies does ETLX use? linkETLX is built primarily in Go and is:\nSQL-first Markdown-driven DBMS Engine-agnostic It is powered by DuckDB for in-process analytics, but also supports:\nPostgreSQL SQLite MySQL SQL Server ODBC-compatible databases ETLX does not introduce a proprietary DSL.\nIs ETLX tied to DuckDB? linkNo.\nDuckDB is the default and recommended engine due to its:\nIn-process execution Multi-source querying Performance However, ETLX is designed to support multiple SQL engines and execution backends. DuckDB is a strength â€” not a lock-in.\nIs ETLX just a runtime? linkNo.\nETLX is:\nA runtime A configuration model An specification The same configuration can be used to:\nExecute workflows Generate documentation Produce governance artifacts like data dictionaries, data lineage, data quality rules, reports, â€¦ How does ETLX handle documentation and governance? linkETLX parses Markdown into a structured model (map[string]any in Go), which includes:\nDataset metadata Column-level definitions Ownership and lineage Validation rules Execution semantics This enables automatic generation of:\nData dictionaries Governance documentation Audit reports Quality checks Without duplicating logic.\nHow can I contribute? linkETLX is community-driven.\nYou can contribute by:\nImproving documentation Adding examples Submitting bug reports Proposing specification improvements Contributing code ðŸ‘‰ Contribution Guide https://realdatadriven.github.io/etlxdocs/docs/contributing/\nWhere is the source code? linkThe source code is hosted on GitHub:\nðŸ‘‰ https://github.com/realdatadriven/etlx\nSupport Questions linkHow can I get help? linkYou can get support by:\nReading the documentation Opening GitHub issues Participating in discussions The community is encouraged to help shape ETLXâ€™s evolution.\nAre there tutorials or examples? linkYes.\nThe documentation includes:\nQuickstart guides Core concepts Advanced examples Real-world patterns ðŸ‘‰ https://realdatadriven.github.io/etlxdocs\nHow do I report a bug? linkPlease open an issue on GitHub:\nðŸ‘‰ https://github.com/realdatadriven/etlx/issues\nInclude:\nA minimal example The ETLX version Expected vs actual behavior Licensing Questions linkWhat license does ETLX use? linkETLX is licensed under the Apache License 2.0.\nThis allows:\nCommercial use Modification Distribution With proper attribution.\nCan ETLX be used commercially? linkYes.\nETLX is explicitly designed to be usable in commercial and enterprise environments.\nHow should ETLX be attributed? linkPlease reference the project as:\nETLX â€” https://github.com/realdatadriven/etlx by RealDataDriven\nFuture \u0026 Roadmap Questions linkIs ETLX stable? linkETLX is actively developed.\nThe core ideas are stable, while the specification is evolving with community feedback.\nAre there upcoming features? linkPlanned and ongoing work includes:\nExpanded open specification More advanced examples Additional export and governance templates Improved observability and validation primitives How can I suggest features? linkFeature suggestions are welcome.\nYou can:\nOpen a GitHub issue Start a discussion Propose changes to the specification Will ETLX receive regular updates? linkYes.\nETLX follows an incremental, transparent development approach. Changes and improvements are tracked openly in GitHub.\n"
            }
        );
    search.addEventListener('input', show_results, true);

    function show_results(){
        const maxResult =  5 ;
        const minlength =  0 ;
        var searchQuery = sanitizeHTML(this.value);
        var results = index.search(searchQuery, {limit: maxResult, enrich: true});

        
        const flatResults = new Map(); 
        for (const result of results.flatMap(r => r.result)) {
        if (flatResults.has(result.doc.href)) continue;
        flatResults.set(result.doc.href, result.doc);
        }

        suggestions.innerHTML = "";
        suggestions.classList.remove('d-none');

        
        if (searchQuery.length < minlength) {
            const minCharMessage = document.createElement('div')
            minCharMessage.innerHTML = `Please type at least <strong>${minlength}</strong> characters`
            minCharMessage.classList.add("suggestion__no-results");
            suggestions.appendChild(minCharMessage);
            return;
        } else {
            
            if (flatResults.size === 0 && searchQuery) {
                const noResultsMessage = document.createElement('div')
                noResultsMessage.innerHTML = "No results for" + ` "<strong>${searchQuery}</strong>"`
                noResultsMessage.classList.add("suggestion__no-results");
                suggestions.appendChild(noResultsMessage);
                return;
            }
        }

        
        for(const [href, doc] of flatResults) {
            const entry = document.createElement('div');
            suggestions.appendChild(entry);

            const a = document.createElement('a');
            a.href = href;
            entry.appendChild(a);

            const title = document.createElement('span');
            title.textContent = doc.title;
            title.classList.add("suggestion__title");
            a.appendChild(title);

            const description = document.createElement('span');
            description.textContent = doc.description;
            description.classList.add("suggestion__description");
            a.appendChild(description);

            suggestions.appendChild(entry);

            if(suggestions.childElementCount == maxResult) break;
        }
    }
    }());
</script></body></html>